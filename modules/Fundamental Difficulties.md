---
id: 4a6836a1-9572-43da-9560-0bcaef788555
slug: module-fundamental-difficulties
title: "Fundamental Difficulties"
discussion: https://discord.com/channels/1440725236843806762/1467932217312547019
tags:
  - module
  - work-in-progress
---
# Page: Welcome
id::
## Text
content::
The AI ​​alignment problem currently has no proven solution. This module provides a unified picture of why achieving alignment is inherently challenging and the fundamental challenges we must overcome to find a safe way to build AI. This is especially true given that, even with competent participants, the default outcome can be poor, as competitive pressures and coordination failures can encourage risky deployments and lock the ecosystem into an equilibrium that none of the participants desire.

In this module you will engage with core concepts including the rocket alignment analogy and the need for formal “alignment math”, why alignment arguments feel counterintuitive, instrumental convergence and power seeking, capabilities generalization and the sharp left turn, the nature and emergence of agency, why misalignment may be the default outcome under current training paradigms, and how coordination failure can overwhelm technical solutions.

Questions you will be able to answer after completing this module:
* Why is it unrealistic to expect we can list all alignment challenges in advance, and what does this imply about the kind of theory we need?
* What does the rocket science and cryptography analogy capture about alignment, and where does the analogy break?
Why do “alignment is hard” arguments feel alien to human intuition, and which recurring intuition traps lead people to underestimate risk?
Under what prerequisites should we expect power seeking to emerge from instrumental convergence style reasoning?
Why do capabilities tend to generalize more reliably than values, and how can this asymmetry produce a sharp left turn?
What does a sharp left turn look like in a simplified model, and what kinds of evidence would not be sufficient to rule it out?
When is it appropriate to model an AI system as an agent?
How can agency emerge from advanced prediction, and how can an “oracle” system gain real world control?
Why might misalignment and catastrophe be the default outcome of scaling current training methods, absent fundamental advances?
Which kinds of “fundamental advances” in alignment or AI theory would actually justify safe deployment, and what failure modes would they rule out?
How can coordination failures and competitive pressure drive unsafe outcomes even when individuals prefer safety, and how does this relate, or not, to instrumental convergence?



# Learning Outcome:
optional:: true
source:: [[../Learning Outcomes/Explain the fundamental difficulties of AI alignment]]

# Learning Outcome:
source:: 

