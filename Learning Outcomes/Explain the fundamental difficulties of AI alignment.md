---
id: d28fc627-50cd-45df-b770-d31831e0c690
discussion: <discord url>
learning-outcome: Participants can explain the reasoning behind the claim that building aligned AI involves predictable, fundamental difficulties.
student-outcome: Participants can apply a “viability test” to AI safety proposals and identify at least two necessary clarifying requirements for ideas that fail to address the fundamental difficulties of alignment.
tags:
  - work-in-progress
  - learning outcome
---
## Test:

## Lens:
optional:: false
source:: [[../Lenses/Eliezer Yudkowsky – AI Alignment- Why It's Hard, and Where to Start]]

## Lens:
optional:: true
source:: [[../Lenses/Lens Template|Lens Template]]