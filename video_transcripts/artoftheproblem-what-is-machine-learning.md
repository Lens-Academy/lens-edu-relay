---
title: "What Is Machine Learning?"
channel: "Art of the Problem"
url: "https://www.youtube.com/watch?v=yLAwDEfzqRw"
---

Imagine you were on the run from machines and you had a choice. You could be chased by Machine A, which was programmed by a 100 humans, but it had a fixed behavior and couldn't learn new things from its experience. Or Machine B, it was programmed by one person with nothing but the ability to learn rapidly to achieve its goals of finding you. Which would you choose?

You could say the field of artificial intelligence began by fearing Machine A, which followed human instructions. During the second half of the 20th century, it became clear that non-learning machines were not as intelligent as we'd hop them to be. There was no unified definition, even among AI researchers. In the last decade or so, the modern field of AI has shifted to this learning-first view of intelligence: how good you are at learning. It views learning as the core mechanism or engine behind what we call intelligent action, where learning is the search for a behavior which achieves a goal, and behavior is what you do and how you react to the environment.

It's also why Machine B, the one that learns from experience, is so scary. It would be constantly adapting to your patterns of behavior and could learn behaviors outside the scope of your understanding. This machine learning paradigm represents a bottom-up view of intelligence inspired by how nature solves problems via learning.

And when we look closely at the natural world, we can identify three distinct layers of learning. Let's start with the first layer: evolutionary learning.

Now, is there a difference between life and intelligent life? If we were looking for intelligence in space, what would we look for? Imagine, for example, we found a simple slim like mold on another planet similar to Earth. Researchers have shown that molds can sense and grow towards food sources in ways that solve complex problems. For example, they exposed slime molds to food locations which mirrored the Tokyo railway station locations to see how efficiently they would grow towards food. What they found was the mold solved the problem as well as humans did by growing an optimal system of lines between the stations, or food. You can't beat this strategy.

So mold exhibits a distributed form of intelligence which is part of the very fabric of life. So let's just focus on life itself. A key feature of what we call life is that which can survive and replicate in the environment. How it achieves this goal is the mark of intelligence.

Imagine we found a single-cell life form, such as this bacterium. When we observe bacteria, we find that they have a swimming behavior which falls into two obvious states: random swimming, where it changes directions randomly, and straight swim. Bacteria are looking for sugar, so you'll find they swim in some random direction and store a memory of the smell at that position. And then it moves forward slightly and compares the smell at that point to its memory. And if the smell is getting stronger, it swims straight. If instead it's getting weaker, it swims in a random direction, and this process repeats. As if it's playing a game of "you're getting warmer or colder" by itself, using smell, climbing towards the strongest smell where the food likely is.

If you think you can do better, give it a try. Close your eyes and imagine, like the bacterium, you were trying to find someone cooking food. Your natural strategy will be to poke your nose around in different directions until you feel an increase in some smell. Once you find the direction of a scent, you'll walk a straight pathway towards the food by walking uphill in the smell space. With all the power of your brain, you can't do better than bacteria given the same information.

This example is important because it shows a pattern behavior behind life: continuously try random things and keep doing what works well, a process of improvement or learning in we call evolution. It's a slow learning process that happens over generations.

For evolution to take place, living things must contain a recipe for how to copy themselves. DNA is a long molecule which stores information for how to make the parts of a life form, and it's written in an alphabet of four chemical symbols. If we blew up our DNA code to the size of written letters, it would look like this. Now, we don't fully understand the information in DNA, but what we do know is some pages in this book book, about 1% of it, act like recipe pages for how to grow a life form. We call these pages genes.

To grow a new life form, these genes are transformed into molecular machinery called proteins, which react with the environment to create the form of an organism. For example, you have a gene which creates blood cells. The point is, our flesh and blood all grows out of the information in our DNA. We begin as a program.

And DNA can begin has nothing but random symbols at first. What matters is that when DNA replicates itself or splits into copies, tiny mistakes are made resulting in mutations to the program, like rolling dice with a few of your genes each time you divide. This results in some changes to the form of the offspring, some good, some bad, all by accident. For example, more legs or longer legs might help a bacterium swim faster, gathering more energy and replicating more than others. And that's the source of the feedback loop which is critical to any learning process.

Life evaluates each copy of DNA via the number of offspring it ends up producing. The offspring who do well will survive and take over, thanks to their copies. It leads to a self-propelling process of improvement. For example, let's say bacteria have genes in their DNA which define their body size as well as swim speed. What's the best size and speed to have? Well, this is a search problem an evolution figur figes this out by randomly sprinkling copies in the space of possibilities, where each life form can be thought of as a point in the space. Now imagine the success of each point, or life form, is measured by the height of this surface or number of offspring. Over generations, the successful points will climb uphill to the optimal configuration via their offspring.

As the environmental conditions change, life can adapt its form over the generations by constantly searching for improvements and saving them as information in the DNA. And one critical behavior that emerges from this process is communication, which is when we have a connection between action and sense. For example, eventually bacteria find themselves in a crowded environment. This leads to a successful mutation which causes bacteria to release a molecule which they can also sense, broadcasting their presence to the environment which nearby bacteria can smell. This allows iia as a whole to sense the total population density in a region so they can respond accordingly. A kind of hive mind emerges which can estimate how many others are around, and the signaling strategy improves their chance of survival.

As unicellular life becomes multicellular life, we also see this idea of outward signaling turn inward to coordinate a body of different parts, moving from external to internal communication. To make multicellular bodies work, it requires the parts of the developing system to talk to each other in order to coordinate action, such as swimming, or rapid reactions such as snapping. For example, the Venus fly trap. It contains a series of sensitive hairs which, if touched, cause it to clamp shut to capture live insects.

The key point is, out of this evolutionary learning process, we see intelligence emerge as hardwired form and function, where fixed behav behaviors are triggered by a fixed sense. But this wind-up-and-go approach is limiting, as the life forms can adapt in the short term to changes within the environment. Their behavior is locked in for life. This leads to the next layer of learning we see grow out of the evolutionary process in life: learning based on experience, resulting in behavior which adapts within a lifetime.

This layer of learning is what we use our brains for. Brains of involved in organisms as bundles of signaling cells that cluster together in groups to form flexible control mechanisms, allowing connections between sense and action to evolve within life the simplest example of this kind of learning is habituation. For example, some plants close up if they are impacted as a form of protection. However, if the harmless impact happens over and over again, such as drops of rain, they begin to ignore it. But eventually, if you keep applying the same touch without damaging it, it can learn to trust the touch too. And we all do this. Imagine again you are in the dark. You're are going to auto fear any new sensation at first, except what you learn to trust. It's the safest way to confront the unknown when each step forward is a mystery. This simple mechanism of habituation, you can think of as a kind of wire-cutting or narrowing of your behavioral triggers. It helps to prevent exhaustion by conserving our energy on issues of survival rather than environmental distractions.

Another kind of learning is the expansion of triggers, which is like adding new wires or connections between sense and action, known as learning by association. The classic experiment to demonstrate this mechanism was to present dogs with the sound of a bell right before feeding. After a few exposures, the bell would cause the dogs to salivate even if no food was presented. This salv behavior was triggered by a sense, the bell, the dog was not born to understand. Automatically, the experimenter created a connection in the dog's brain through the experiment.

The mechanism behind association is rapid changes in chemical reward signals in our brain, such as dopamine, which can shape our pattern of behavior. An unexpected positive stimulus, such as food, can trigger a burst of chemicals which help positively reinforce the connection to our senses, especially senses which are close in time to the stimulus. While an unexpected negative stimulus, such as getting shocked, will result in a decrease in dopamine, which will negatively reinforce the connections to those senses instead.

Ignoring things, habituation, or paying attention to new things, association, allows the mind to shape our sense to match our environment, which enable better predictions about how to behave in the world. This results in what you can think of as an adaptive sense, which is the ability to change how a set of behaviors are triggered.

And this helps, but it doesn't explain the active side of things, which is adapting our actions to match the environment too. To do this, evolution has found a clever strategy. We are born clumsy with a kind of semi-finished or random wiring in our brain, only kind of knowing the shape of what to do. And to get better at things, we just add experience or play, which are random actions which fine-tune our behavior or wiring until it feels right. This is what we commonly call trial-and-error learning or reinforcement learning, where we try random things and learn from what happens.

It's a form of association where the brain remembers the pattern of behavior leading up to a stimulus and does more or less of that behavior as a result. The classic experiment to show this mechanism is teaching birds or rats or octopuses to push a button or pull a lever to get a reward. And this feedback loop between behavior and stimulus can be very long or very short. Some life forms are able to learn more complex or longer behavior patterns than others.

It's important to know that the engine of this kind of reinforcement learning is experience, and that can be boosted by play. Play allows animals to test out or simulate behaviors without the consequences. So the key difference between evolutionary learning and learning in life with a brain is it allows learning at a faster time scale, and this leads to complex learn behavior in life that genetics alone wouldn't be able to provide in advance.

And there is one more critical layer of learning to consider, and it gets us closer to what we call human-only intelligence. Recently, we discovered a clue in our DNA which explains what might have happened around 3 million years ago. A mutation occurred which activated a gene called notch 2nl, which extended the growth period of the brain, which led to brains which tripled in size while other brains didn't. So what do these big brains do?

It's clear now that, among other things, a new kind of thought emerges, a mental superpower we all have. It's the imagination, allowing us to learn with our thoughts, what we could call abstract learning. It allows us to imagine situations we haven't experienced and gauge what will happen so we can adapt our behavior accordingly. It's a faster and risk-free form of inlife learning.

But this form of learning we know the least about. Brain imaging studies gives us a hint of what's going on at this abstract level. We find that when people are asked to imagine things, it fires an activation pattern in their brain which reaches out of the sensor motor regions into a new, highly connected upper layer of the mind which is decoupled from direct experience. You can think of this as our simulation layer, and it's a newer kind of thought from an evolutionary perspective that we all have.

And we can really see this distinction between direct and abstract in life learning when we turn back to communication. For example, monkeys signal each other to know that a predator is near, while bees will dance together to signal the direction an amount of nearby food. And this is the basis of language. Language allows mind to synchronize.

But it's human language which jumps out as very different. When we share symbols or gestures to communicate, we do it in much longer sequences than animals. Animals tend to say a fixed set of short things, no matter how hard we teach them, while humans just go on and on. These longer sequences allow us to share arbitrary mental simulations or stories, allowing the mental simulation going on in our mind to leap out into another. It's quite remarkable that humans can utter some sounds and cause another mind to learn something new with no action taking place.

And the capacity for language leads to huge advantages once it begins to accumulate over generations, each mind holding the learning of thousands of years past, leading to what we call a culture of behaviors and technologies of extraordinary complexity, far exceeding what any single human mind could comprehend or develop on their own. This is why language has always been the marker of higher-level intelligence we hope to find in space.

It's also why the classical top-down view of AI was always language-first, which led us to the symbolic view where we program our machines by telling them stories about how to be smart. And this approach was limited by our own imaginations about how we think we think, and more importantly, how complex the world really is.

And that's why the modern view is instead bottom-up, where we look at intelligent action as a result of some learning process, where the computer is given a sense, an ability to act, and then we see if it can learn to achieve goals we provide. So this learning process started in our genes, then rooted in our experience, and finally was set free in our minds, which leaves us with some interesting question questions: What form should our learning machines take, and who or what defines its goals?

[Music]
