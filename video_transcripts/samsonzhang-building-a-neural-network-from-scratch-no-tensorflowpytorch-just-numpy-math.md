---
title: "Building a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy & math)"
channel: "Samson Zhang"
url: "https://www.youtube.com/watch?v=w8yWXqWQYmU"
---

Hi everyone, my name is Samson. Today I'm going to be building a neural network from scratch, so not using TensorFlow, not using Keras, just NumPy with equations, linear algebra, from the ground up. Anyone interested in artificial intelligence or machine learning is probably very familiar with neural networks at a high level, right? You have lots of layers, lots of nodes. You connect them all together. You can have some really complex models from it that make some cool predictions, right?

I find that a lot of that kind of learning, right, when you're just looking at stuff from the high level, is kind of wishy-washy. And even if you go into TensorFlow and implement these networks, it's still a little unclear, like how they work, you know? At least for me, I feel like I learn a lot better when I can get to the equations, when I can really build from the ground up. And you really can't get too much closer than implementing a neural network from scratch. So that's what we're going to be doing today. So let's dive right into it.

The problem that we'll be tackling is digit classification. We'll be using a famous dataset called the MNIST dataset. So what the MNIST dataset is, is it's tens of thousands of these 28 by 28, so pretty low res, grayscale images of handwritten digits. We are going to be building a neural network that classifies images of handwritten digits and tells you what digit is written in that image.

This is an overview of what everything is going to look like, what we're going to implement today. We're going to start off with 28 by 28 pixel training images. So that's 784 pixels overall, and each of those pixels is just a pixel value, right? It's between 0 and 255. 255 being completely white, 0 being completely black. So we have m of these training images, so we can represent it as a matrix that looks like this. Each row constitutes an example, and each row is going to be 784 columns long because each of them is going to correspond to one pixel in that image.

What we're actually going to do is transpose this matrix, instead of each row being an example, each column is an example. So our first column is going to be our first example, and it's going to have 784 rows corresponding to each pixel, and we're going to have m columns corresponding to our m training examples. Our goal is to take this image, just do a bunch of processing, and then predict, give out a prediction for what digit that image represents. And how we're going to do that is with a neural network.

So we're going to be building a quite simple neural network. It's only going to have two layers. So the first layer in this neural network is just going to be 784 nodes. This is our input layer, right? Each of the 784 pixels maps to a node, and that's our first layer. Our second layer is going to be our hidden layer, and it's going to have 10 units. And then our third layer is going to be our output layer, again with 10 units, and each corresponding to one digit that can be predicted.

In that intro right there, I called this the first layer, this the second layer, and this the third layer. Going forward, I'm actually calling this right here the zeroth layer. This is the first layer, and this the second layer. And the reason for that is this layer right here will also be called the input layer. There's no parameters here, right? This is not really a layer of the network. It's just the input. This right here is the first hidden layer, and this is our second layer and also our output layer. So yeah, so I just called them first, second, and third, but going forward we're going to be not using first, second, and third, but rather input layer, first hidden layer, and second layer, otherwise known as output layer.

There's three parts to training this network. The first part is called forward propagation. So forward propagation is just when you take an image and you run it through this network. From this network you compute what your output is going to be. So that's the first part that we have to figure out. So to start, we're going to have this variable a0. That's just our input layer. That's just equal to x. There's no processing going on here. A0 is just this first layer right here.

z1 is the unactivated first layer. What we're going to do to get z1 is apply a weight and a bias. So a weight is going to be this matrix. We're going to take the dot product between that matrix and a0, our input layer matrix, and that's going to give us something. And then we're going to add just a bias term to it. What we're doing here is really multiplying by a bunch of weights that correspond to each of these connections, each of these, what's 7,840 connections. And then we're going to add a constant bias term to each of the nodes.

We're going to do after that is kind of interesting. We're going to apply an activation function to it. So if we didn't apply an activation function, what would happen is that each node would just be a linear combination of the nodes before it, right, plus a bias term. The second layer is going to be a linear combination of the nodes in the first layer, but the first layer is just a linear combination of the nodes in the input layer. So what's really happened is that that second layer is just a linear combination of the input layer, right? It's like you don't have a hidden layer at all.

So if you only have linear combinations, if you only have your weights and your biases, you're never going to get a really interesting function out of neural network. Really you're just doing a really fancy linear regression. To solve that, we apply an activation function. There's a bunch of like common ones. So tanh and sigmoid, I'm sure you've heard of them. They kind of look like this. You know, they got your nice curves, and that's going to make it more interesting. If you apply a tanh function or a sigmoid function to all of the nodes in a layer, it's no longer linear, right? When you move on to the second layer, that's adding now a layer of complexity and a layer of interesting non-linear combination rather than just a linear model. And that's going to make your model able to be a lot more complex and more powerful.

We're going to be using another really commonly used activation function called ReLU, or Rectified Linear Unit. So ReLU just looks like this. It's really simple and how it works. So ReLU of x is defined as: if x is greater than zero, ReLU of x is just equal to x. It's literally linear. And if x is less than zero, or less than or equal to zero, it doesn't really matter, it's equal to zero. The value is equal to zero. So it's just this really simple function, but just that's going to add the complexity that we need.

a1 is going to be equal to the ReLU function applied to every value in z1. Now we're going to do kind of a similar thing to get from layer 1 to layer 2. So our z2, right, so our unactivated second layer values, is going to be equal to a second weight parameter, right, weights corresponding to the values of the weights on each of these connections here between the first and the second layers, times our a1, times the activated first layer, plus another constant bias term.

This time we're going to apply another activation function, but the activation function we apply this time is not going to be a ReLU or sigmoid or tanh or anything like that. It's going to be something quite different. It's called softmax. Because this is an output layer, right? Each of the 10 nodes corresponds to each of the 10 digits that could be recognized. We want each of them to have a probability, right? We want each of them to be a value between 0 and 1, where 1 is absolute certainty and zero is no chance at that. That's what it is. So to get that, we use a softmax function.

So I've just copy-pasted an image here. I didn't feel like writing this out myself. It takes each of the nodes in the layer that you feed into it, and it goes e to the z, so e to that node, divided by the sum of e to all the nodes, right, or rather the sum over all the nodes of e to that node. Each of your outputs after the softmax activation is going to be between 0 and 1, right, because e to a single node is always just going to be a part of the whole that is the sum of e to all the nodes.

So forward propagation is how you take an image and get a prediction out, but that's not enough, right? We need good weights and biases to make these predictions. And the whole thing about machine learning is that we will learn these weights and biases, right? We will run an algorithm to optimize these weights and biases. As we run it over and over again, we're going to do that in something called back prop, backwards propagation. And what we're doing here is basically going the opposite way. So we're going to start with our prediction, and we're going to find out how much the prediction deviated by the actual label, right? So that's going to give us a sort of error. And then we're going to see how much each of the previous weights and biases contributed to that error, and then we're going to adjust those things accordingly.

So dz2 represents sort of an error of the second layer, writes how much the output layer is off by. And this is really simple. We just take our predictions and we subtract the actual labels from them. And we're going to one-hot encode the correct label. So for example, if y equals four, we're not going to subtract four from this, right? We're going to encode y equals four into this array here, where the fourth index, right, representing the fourth class, is a one. Everything else has a zero.

From there, we do some fancy math to figure out how much w and b contributed to that error. So we can find this variable dw2 that is the derivative of the loss function with respect to the weights in layer 2. db2, this one is really easy to understand. What this literally is is an average of the absolute error, right, literally just the error, how much the output was off by. That's for the second layer, right? We're going from, we're finding how much the second layer was off by from the prediction, and then we find out correspondingly how much we should nudge our weight and our biases for this layer.

Now we'll do the same thing for the first hidden layer, but we're going to find dz1, right, how much that hidden layer was off by. And here there's a fancy bit of math here that's the intuition is kind of that it's just doing propagation in reverse, right? Here you see this weight for the second term transpose times dz2, right? We're taking the error from that second layer and applying the weights to it in reverse to get to the errors for the first layer. And we're also taking this g prime here. That's the derivative of the activation function, because we have to undo the activation function to get the proper error for the first layer. And then we do the same thing as we did before to calculate how much w1 and b1 contributed to the error in the first layer, and that's how much we're going to nudge them by.

So yeah, so after we do all this fancy calculation and figure out how much each weight term, each bias term contributed to the error, we update our parameters accordingly. So this is a set of pretty simple equations, right? w1 is equal to w1 minus alpha, some learning rate alpha, times dw1. Similarly, b1 is b1 minus alpha times db1. w2 is w2 minus alpha times dw2, and b2 is b2 minus alpha times db2.

Alpha is what's called a hyperparameter. It's not trained by the model. The, when you run this cycle, when you run gradient descent, the learning rate is a parameter that you set, not that gradient descent set. So once we've updated it, we run through the whole thing again. We go through forward prop. We make a new round of predictions. We are changing our parameters, tweaking them so that the prediction is ever closer to what the actual correct answer should be. That's math. Now let's get to coding it up.

I'm going to attempt to do it in the next 30 minutes. We're going to be doing this on a site called Kaggle. Kaggle's this really great site that makes it really easy to access datasets and have notebooks, have Python notebooks, Jupyter notebooks. So our digit recognizer dataset is already nicely configured here, and all we have to do is hit New Notebook, and we will be taken into a kernel. Here we are. We have a notebook, and let's just rename it real quick.

So the first thing that we're going to do is import our package. So NumPy is for linear algebra, for like working with matrices, and then pandas is just for reading the data. We're not going to use it much. So import. Next, let's import the data. So we're going to use pandas for this. So data is going to be equal to pandas.read_csv. We're going to specify the file path. So you can see how Kaggle makes it super easy to access the data here. Because it's pandas, it loads it as a pandas DataFrame. We can call head on it and get a little preview of what's up here. Each row corresponds to one training example. They have a label, and they also have all these pixel values. For each example, we have pixel 0 all the way to pixel 783. So inclusive, that is 784 pixels.

We actually don't want to be working with a pandas DataFrame. We want to be working with NumPy arrays so that we can manipulate them and do fancy linear algebra with them. So we are going to say that data is equal to np.array of data. And then we are going to split up our data into dev and training sets. There's always a risk of overfitting, right? There's a risk that your model is going to figure out exactly how to make exactly the right predictions for your training data but not generalize to the data that you actually want to be generalizing to. So you set aside a chunk of data that you don't train on, and that's your dev or your cross validation data. And that way you can test your hyperparameters on that data. You can test your performance on that data. You eliminate the risk of overfitting to your actual training data.

We're going to shuffle our data, so hopefully it's just np.random.shuffle. Okay, hopefully I'm not going crazy here. We're going to do one thing before that. We're going to get the dimensions of the data. So m and n is going to be called data.shape. So m is just the amount of rows, right? That's the amount of examples that we have. And n is not quite the amount of features. It's the amount of features plus one because we have this label column, but it'll be helpful to have that.

So there we go. And now we're going to go into the dev is going to be equal to data from zero to a thousand. Okay, the first thousand examples, and then I want all of it, so I'm going to go with that. Okay, let's transpose this as well. Okay, so we're going to transpose it. So if you remember, that's the thing where we flip it so that each column is an example rather than each row, and that just makes it easier here.

y_dev, if you remember, is now going to be the first row, right? It's going to be super convenient. y_dev, damn, what am I doing? data_dev[0]. And x_dev is going to be data_dev 1 to n. Remember that, that's our n coming in handy here. And then our data_train, this is the data we're actually training on, it's going to be zero, sorry, 1000 to m, so all the rest of it. Transpose it again. y_train is going to equal the data_train at zero, and then x changes, data_train from one to n. Beautiful. So this should be all of our data loaded in. Let's run that and take a look at, for example, y_train. Okay, y_train. Look at that. So we have an array of all of our labels.

And then let's take a look at x_train zero. That's like the shape. It's not super helpful. Look at that. That's looking at the first row, so that's actually not what I want. I believe I want this. There we go. So there we go, that's our first column, and it has 784 pixels in it. That's exactly what we want. So okay, now we have our data. Now we can start just bashing out code for the actual neural network part. Let's see how much time we spent. Okay, seven minutes. We'll move fast. We'll move fast.

The first thing we're going to do is initialize all of our parameters. We need a starting w1, b1, w2, b2, right, and we have all the dimensions for them here. So we're going to go def init_params. And this takes no arguments. It doesn't need any arguments for that function because it's creating the complete from scratch. w1 is np.random, and I think is it that? np.random.randn. I believe it's randn. Okay, and our first, and we want the dimensions of this array to be w1 is going to be 10 by 784. This is going to generate random values between 0 and 1 for each element of this array. So we're actually subtract 0.5 from that to get it between negative 0.5 and 0.5.

b1 is going to be np.random. The dimensions here are going to be 10 by 1, and again we're going to subtract 0.5 from that. And then same exact thing for w2. Okay, so w2 is going to be 10 by 10, and b2 is going to be that. And then we're going to return all these values. Okay.

After we initialize the params, we are basically ready to go into forward propagation. So we're going to go def forward_prop, and this is going to take w1, b1, w2, b2 as arguments, and also x. We're also going to need x. So first we're going to calculate, let's calculate z1. Okay, so z1 is going to be equal to w1.dot. So remember, w1 is a NumPy array, so we can do matrix operations using this dot, dot, dot x. Okay, plus b1.

Now a1 is going to be equal to ReLU of z1, but ReLU is not defined, so let's define ReLU here. ReLU of z. And remember, ReLU is just this linear function right here, right? It's going to be x if x is greater than zero and zero if x is less than or equal to zero. There's a pretty elegant way to do this. That's np.maximum. I'm going to take the max of zero and z, and this is element-wise. So when we take maximum like this, what it's doing is it's going through each element in z. If it's greater than zero, it's just going to return z, right? But if it's less than zero, it's just going to return zero. So this is exactly what we want. We'll return that.

So a1 is going to equal to ReLU of z1. That's exactly what we want. Great. And now we'll do z2. z2 is going to be equal to w2.dot a1 now plus b2. And then a2 is going to be equal to softmax of a1. And again, we need to define softmax now. Softmax of z. And we're going to return here, it's going to be, we're going to reference this formula right here, and what we can actually do is np.exp. And remember, that's just like, that's just e to the x, right? And we're doing that to each element of the array. So we're going to do this divided by np.sum of np across z.

First, it just applies like e to the z to every single value, and then np.sum is going to sum up through each column, so it preserves the amount of columns and then collapses the amount of rows to just one to get this sum, right? And that's because, and that's exactly what we want. We want the sum for each column across all of the rows in that column, and we want to divide each element by that sum. And that's going to give us the probability that we want. So there we go, that's forward prop. Forward prop is done.

Now in backprop, we are going to take in, let's just take in all of these. Okay, beautiful. So the first thing that we're going to do is we actually need to do one thing to y. We need to one-hot encode y. We need to take these labels and turn it into this, this matrix thing here, remember? So oops, we're going to define a function one_hot. Okay, and it's going to take in y. And here I've actually cheated a little bit, and I've taken one-hot from when I did this previously.

What this is doing is first it creates this new matrix one_hot_y, right, which is just m zeros. It's an array, a matrix of zeros, and this is a tuple of size, right? So y.size is just a length of size, so this is m, right? This is how many examples there are. And then y.max plus one. It assumes that the classes are, you know, zero through nine, right? So the max is going to be nine, and then we add one to that, and we get 10, which is exactly how many output classes we want. So this creates the correctly sized matrix for us.

And this is a really cool thing. You're indexing through one_hot_y using arrays, right? np.arange y.size. This is going to create an array, right, that's just a range from zero to m, right, to the number of training examples. And that specifies what row this is accessing, and then y, remember, is our whole thing of labels, right? This is going to be like 0, 1, 4, 1, 7, whatever. This is going to specify what column it accesses. So what this is doing is it's just going through and it's saying, for each row, go to the column specified by the label in y and set it to one, right? And that's beautiful. That's exactly what we want. That'll do the one-hot encoding for us.

And the one last thing we want to do is just flip this. Okay, one_hot_y.T, and then we can return one_hot_y. And we want to flip it because right now, right now each row is an example, and we want it the other way around. We want each column to be an example, so we'll transpose it and return it.

Back here, so perfect. Now we're going to go one_hot_y is equal to one_hot of y, which you look at that. And dz2, okay, this is a new variable, is equal to a2, our predictions, minus one_hot_y. dw2 now is going to be equal to 1 over m. Okay, let's define m. m is going to be y.size, okay, just like we did before. 1 over m times dz, oops, dz2. Looks like this is a, dz2.a1.T. Yeah, I believe that's right. I believe that's right.

Next, db2 is, db2 is just 1 over m times np.sum of dz2. And then we want our dz1, which is going to be our fancy formula here. It's going to be w2.transpose.dot dotted with dz2. So this is kind of applying the weights in reverse. And now we're going to have to implement the derivative of the activation function in layer one. And the activation function in layer one, remember, was ReLU. So we're going to implement the derivative of ReLU.

And that seems kind of fancy at first, and I was freaked out when I was, I was first, you know, I was like, how, how on earth am I supposed to do this? But it's actually really easy. Just think about your calculus, right? What is the slope of this, this part here? It's one, right? It's just a linear thing. What's the slope of this part here? It's a zero, right? It's a flat line. So a really elegant way to do this is just return z is greater than zero. Well, this works because when booleans are converted to numbers, true converts to one and false converts to zero. If one element in z is greater than zero, we're going to return one, and if otherwise we'll return zero, which is exactly the derivative that we want. Beautiful.

And then now the same thing as, same thing as we did before. Oops. I believe it's, it's actually going to want x as well, because we're going to have x.T, x.T here. dz1, oh my goodness, that was not what I wanted to do. Come on. There we go. And then from here we're going to return dw1, db1, dw2, db2.

And we go def, def update_params. Now, all right, we're all the way down to update_params, and we're going to take in here dw1, db1, dw2, db2. And actually, before that, I'm just going to take in w1, b1, w2, b2, and then alpha, right? I'm going to go w1 is equal to w1, this part is really straightforward, right, times alpha times dw1. b1 equals b1 minus alpha times db1, and then we'll do the same thing for two. And then we're going to return our new w1, b1, w2, b2. Perfect.

So in theory, if that's correctly implemented, that is all of the functions that we need for doing our gradient descent on our neural network. Def gradient_descent. That's what we're going to do now. We're going to take x and y first, because we need both of those, and then we're going to take iterations, alpha, and I believe that's all that we need.

So first, we're going to go w1, b1, w2, b2 is equal to init_params. Okay, that's going to return us these are our initial values. And now we're going to, we're going to run that loop we were talking about, right? So for i in range iterations, first one, go forward prop, right? z1, a1, z2, a2 equals forward_prop, and we're going to pass in w1, b1, w2, b2.

Next, we're going to get our dw1, db1, dw2, db2 from backprop. And then lastly, we're going to update our weights, right? When I say w1, b1, w2, b2 is now going to equal to update_params w1, b1, w2, b2, dw1, db1, dw2, db2, and alpha. And that's going to run a bunch of times, and by the end of it, it will return the params that we want.

We're going to make a few things just to, to, so that we can actually see our progress. I'm going to cheat a bit and check my, check the time that I did this last. We're just going to steal these two functions here. All right, we're going to do here is if i mod 10 is equal to zero, so every tenth iteration, we are going to print iteration, and i, and then print accuracy. Oops, it's going to be a little ugly. Get predictions a2. All right, so we're going to take our a2, which is our predictions from forward prop, and get predictions from those, and then get accuracy on that and y. Okay, and then it'll return that.

So let's run that. So all those functions are defined now. Now let's run gradient descent and see what happens. And I'll take these parameters w1, b1, w2, b2 is equal to gradient descent x, y. Iterations, oh, x_train, y_train. Now we're using the actual variables, and we're going to go for, let's say, 100 iterations with a learning rate of 0.1.

[Editing Note: Samson here. So because of those two errors early in the video, namely initializing weights and biases to all negative numbers and putting in an a1 where there should have been a z2, of course the model did not run well. It ended up with 10% accuracy, which is just random guessing. And I spent like an hour just debugging. In the end, just to change those two variables, right? All the rest of the code is okay. I didn't change any of that. I'm just going to skip over all of that in editing, and we're going to go straight to the end, straight to an hour later when I figured out what was wrong and finally ran the model and go.]

Here.

But look at that. 75% accuracy. 78 at iteration 250. 84% accuracy on training data is what we got. So there's definitely a lot of room left for improvement, things like adding more layers, adding more units per layers. But 84 for this, for something that we threw together in less than 30 minutes is not bad. It's not bad.

So let's try out a few things. Let's now, and I'm just going to rip code straight off from my old thing now. This is just a function to make that prediction and then print out the prediction and label and then display the image. So once we've defined that, let's do test prediction. Let's test out, let's just random number, doesn't matter. If you want to, b2. So bang, it's zero. Our model predicts a zero, and the label is zero. Let's do the second one. So there we go.

Let's do this one. So yeah, so this is a tricky one, you know? This is a weird, you could see, maybe it's a four or maybe it's a five or so, I don't know, something weird. But no, the model did it. It's an eight, and the label is an eight. I need to find one that it mislabels, because the mislabels are pretty interesting as well.

Okay, we are going sequentially, and we're not finding one that's mislabeled. It's doing really well.

Here we go. Here's one. Okay, yeah, so this is a jank three, and it labeled it a five, right? Because you can definitely see, like, just like put the thing here, and it's like a perfect five. This is the kind of thing that if you had more layers, if you had more units, you'd be able to recognize it better that this model wasn't quite complex enough to capture that, like, basically like a 45-degree rotation here. But you know, we went through a good amount without finding one that was mislabeled. So this is definitely a model that actually worked.

Just to do the last step, I want to check with the cross validation accuracy on this is x_dev, w1, b1, w2, b2. Okay, and then dev_predictions. Okay, let's run that through and, okay, look at that. That's an 86, 85.5% accuracy on the dev set. So that's not the training data. We didn't train on this data. This is effectively testing data, right? We haven't done any optimization for this data, and 85.5% accuracy. That's pretty good.

There you go. We've done it. We've done it. We built a neural network from scratch, and maybe watching me code through it, watching me explain those equations, hopefully it helped a bit. I'll have a link in the description to an article, a blog post that I'll put up just with all these notes. I'll have a link to this notebook so you can look through all the code, you can look through all the equations, you can figure everything out for yourself if you so desire.

There's a lot of other stuff to explore implementing manually, right? So things like regularization, different optimization methods, right? So instead of just gradient descent, there's gradient descent with momentum, there's RMSprop, there's Adam optimization, which are all just variants of gradient descent, and they're fairly simple to implement, but it could be interesting just to implement that yourself and see what the impact is.

Yeah, this is a fun exercise. It's always satisfying to see the accuracy go up like that. It's satisfying to see on like a model that you train using like Keras, TensorFlow, and it's all the more satisfying to see on a model that you built yourself. That'll be it for this video, just a simple demo of how you can build out all the math that's on this screen here. I hope this gave you a more concrete understanding of how neural networks work, maybe piqued your interest to dive into the math and do some more yourself. Thanks for watching this video, and I'll see you guys in future videos if you decide to stick around.
