[
  {
    "text": "Imagine you happen across a short movie script that",
    "start": "0:01.14"
  },
  {
    "text": "describes a scene between a person and their AI assistant.",
    "start": "0:03.98"
  },
  {
    "text": "The script has what the person asks the AI, but the AI's response has been torn off.",
    "start": "0:07.48"
  },
  {
    "text": "Suppose you also have this powerful magical machine that can take",
    "start": "0:13.06"
  },
  {
    "text": "any text and provide a sensible prediction of what word comes next.",
    "start": "0:16.98"
  },
  {
    "text": "You could then finish the script by feeding in what you have to the machine,",
    "start": "0:21.50"
  },
  {
    "text": "seeing what it would predict to start the AI's answer,",
    "start": "0:25.51"
  },
  {
    "text": "and then repeating this over and over with a growing script completing the dialogue.",
    "start": "0:28.37"
  },
  {
    "text": "When you interact with a chatbot, this is exactly what's happening.",
    "start": "0:33.38"
  },
  {
    "text": "A large language model is a sophisticated mathematical function",
    "start": "0:37.02"
  },
  {
    "text": "that predicts what word comes next for any piece of text.",
    "start": "0:40.70"
  },
  {
    "text": "Instead of predicting one word with certainty, though,",
    "start": "0:44.38"
  },
  {
    "text": "what it does is assign a probability to all possible next words.",
    "start": "0:47.40"
  },
  {
    "text": "To build a chatbot, you lay out some text that describes an interaction between a user",
    "start": "0:51.62"
  },
  {
    "text": "and a hypothetical AI assistant, add on whatever the user types in as the first part of",
    "start": "0:56.80"
  },
  {
    "text": "the interaction, and then have the model repeatedly predict the next word that such a",
    "start": "1:02.04"
  },
  {
    "text": "hypothetical AI assistant would say in response, and that's what's presented to the user.",
    "start": "1:07.16"
  },
  {
    "text": "In doing this, the output tends to look a lot more natural if",
    "start": "1:13.08"
  },
  {
    "text": "you allow it to select less likely words along the way at random.",
    "start": "1:16.21"
  },
  {
    "text": "So what this means is even though the model itself is deterministic,",
    "start": "1:20.14"
  },
  {
    "text": "a given prompt typically gives a different answer each time it's run.",
    "start": "1:23.62"
  },
  {
    "text": "Models learn how to make these predictions by processing an enormous amount of text,",
    "start": "1:28.04"
  },
  {
    "text": "typically pulled from the internet.",
    "start": "1:32.33"
  },
  {
    "text": "For a standard human to read the amount of text that was used to train GPT-3,",
    "start": "1:34.10"
  },
  {
    "text": "for example, if they read non-stop 24-7, it would take over 2,600 years.",
    "start": "1:39.47"
  },
  {
    "text": "Larger models since then train on much, much more.",
    "start": "1:44.72"
  },
  {
    "text": "You can think of training a little bit like tuning the dials on a big machine.",
    "start": "1:48.20"
  },
  {
    "text": "The way that a language model behaves is entirely determined by these",
    "start": "1:52.28"
  },
  {
    "text": "many different continuous values, usually called parameters or weights.",
    "start": "1:56.30"
  },
  {
    "text": "Changing those parameters will change the probabilities",
    "start": "2:01.02"
  },
  {
    "text": "that the model gives for the next word on a given input.",
    "start": "2:04.10"
  },
  {
    "text": "What puts the large in large language model is how",
    "start": "2:07.86"
  },
  {
    "text": "they can have hundreds of billions of these parameters.",
    "start": "2:10.73"
  },
  {
    "text": "No human ever deliberately sets those parameters.",
    "start": "2:15.20"
  },
  {
    "text": "Instead, they begin at random, meaning the model just outputs gibberish,",
    "start": "2:18.44"
  },
  {
    "text": "but they're repeatedly refined based on many example pieces of text.",
    "start": "2:22.64"
  },
  {
    "text": "One of these training examples could be just a handful of words,",
    "start": "2:27.14"
  },
  {
    "text": "or it could be thousands, but in either case, the way this works is to",
    "start": "2:30.66"
  },
  {
    "text": "pass in all but the last word from that example into the model and",
    "start": "2:34.50"
  },
  {
    "text": "compare the prediction that it makes with the true last word from the example.",
    "start": "2:38.12"
  },
  {
    "text": "An algorithm called backpropagation is used to tweak all of the parameters",
    "start": "2:43.26"
  },
  {
    "text": "in such a way that it makes the model a little more likely to choose",
    "start": "2:47.39"
  },
  {
    "text": "the true last word and a little less likely to choose all the others.",
    "start": "2:51.20"
  },
  {
    "text": "When you do this for many, many trillions of examples,",
    "start": "2:55.74"
  },
  {
    "text": "not only does the model start to give more accurate predictions on the training data,",
    "start": "2:58.75"
  },
  {
    "text": "but it also starts to make more reasonable predictions on text that it's never",
    "start": "3:03.46"
  },
  {
    "text": "seen before.",
    "start": "3:07.78"
  },
  {
    "text": "Given the huge number of parameters and the enormous amount of training data,",
    "start": "3:09.42"
  },
  {
    "text": "the scale of computation involved in training a large language model is mind-boggling.",
    "start": "3:13.92"
  },
  {
    "text": "To illustrate, imagine that you could perform one",
    "start": "3:19.60"
  },
  {
    "text": "billion additions and multiplications every single second.",
    "start": "3:22.28"
  },
  {
    "text": "How long do you think it would take for you to do all of the",
    "start": "3:26.06"
  },
  {
    "text": "operations involved in training the largest language models?",
    "start": "3:29.33"
  },
  {
    "text": "Do you think it would take a year?",
    "start": "3:33.46"
  },
  {
    "text": "Maybe something like 10,000 years?",
    "start": "3:36.04"
  },
  {
    "text": "The answer is actually much more than that.",
    "start": "3:39.02"
  },
  {
    "text": "It's well over 100 million years.",
    "start": "3:41.12"
  },
  {
    "text": "This is only part of the story, though.",
    "start": "3:45.52"
  },
  {
    "text": "This whole process is called pre-training.",
    "start": "3:47.54"
  },
  {
    "text": "The goal of auto-completing a random passage of text from the",
    "start": "3:49.50"
  },
  {
    "text": "internet is very different from the goal of being a good AI assistant.",
    "start": "3:52.65"
  },
  {
    "text": "To address this, chatbots undergo another type of training,",
    "start": "3:56.88"
  },
  {
    "text": "just as important, called reinforcement learning with human feedback.",
    "start": "4:00.08"
  },
  {
    "text": "Workers flag unhelpful or problematic predictions,",
    "start": "4:04.48"
  },
  {
    "text": "and their corrections further change the model's parameters,",
    "start": "4:07.50"
  },
  {
    "text": "making them more likely to give predictions that users prefer.",
    "start": "4:11.11"
  },
  {
    "text": "Looking back at the pre-training, though, this staggering amount of",
    "start": "4:14.78"
  },
  {
    "text": "computation is only made possible by using special computer chips that",
    "start": "4:18.86"
  },
  {
    "text": "are optimized for running many operations in parallel, known as GPUs.",
    "start": "4:23.12"
  },
  {
    "text": "However, not all language models can be easily parallelized.",
    "start": "4:28.12"
  },
  {
    "text": "Prior to 2017, most language models would process text one word at a time,",
    "start": "4:32.08"
  },
  {
    "text": "but then a team of researchers at Google introduced a new model known as the transformer.",
    "start": "4:36.82"
  },
  {
    "text": "Transformers don't read text from the start to the finish.",
    "start": "4:43.30"
  },
  {
    "text": "They soak it all in at once, in parallel.",
    "start": "4:46.75"
  },
  {
    "text": "The very first step inside a transformer, and most other language models for that matter,",
    "start": "4:49.90"
  },
  {
    "text": "is to associate each word with a long list of numbers.",
    "start": "4:54.60"
  },
  {
    "text": "The reason for this is that the training process only works with continuous values,",
    "start": "4:57.86"
  },
  {
    "text": "so you have to somehow encode language using numbers,",
    "start": "5:02.40"
  },
  {
    "text": "and each of these lists of numbers may somehow encode the meaning of the",
    "start": "5:05.31"
  },
  {
    "text": "corresponding word.",
    "start": "5:09.25"
  },
  {
    "text": "What makes transformers unique is their reliance",
    "start": "5:10.28"
  },
  {
    "text": "on a special operation known as attention.",
    "start": "5:13.36"
  },
  {
    "text": "This operation gives all of these lists of numbers a chance to talk to one another",
    "start": "5:16.98"
  },
  {
    "text": "and refine the meanings they encode based on the context around, all done in parallel.",
    "start": "5:21.68"
  },
  {
    "text": "For example, the numbers encoding the word bank might be changed based on the",
    "start": "5:27.40"
  },
  {
    "text": "context surrounding it to somehow encode the more specific notion of a riverbank.",
    "start": "5:31.71"
  },
  {
    "text": "Transformers typically also include a second type of operation known",
    "start": "5:37.28"
  },
  {
    "text": "as a feed-forward neural network, and this gives the model extra",
    "start": "5:41.03"
  },
  {
    "text": "capacity to store more patterns about language learned during training.",
    "start": "5:44.56"
  },
  {
    "text": "All of this data repeatedly flows through many different iterations of",
    "start": "5:49.28"
  },
  {
    "text": "these two fundamental operations, and as it does so,",
    "start": "5:53.40"
  },
  {
    "text": "the hope is that each list of numbers is enriched to encode whatever",
    "start": "5:56.48"
  },
  {
    "text": "information might be needed to make an accurate prediction of what word",
    "start": "6:00.48"
  },
  {
    "text": "follows in the passage.",
    "start": "6:04.66"
  },
  {
    "text": "At the end, one final function is performed on the last vector in this sequence,",
    "start": "6:07.00"
  },
  {
    "text": "which now has had a chance to be influenced by all the other context from the input text,",
    "start": "6:11.53"
  },
  {
    "text": "as well as everything the model learned during training,",
    "start": "6:16.57"
  },
  {
    "text": "to produce a prediction of the next word.",
    "start": "6:19.76"
  },
  {
    "text": "Again, the model's prediction looks like a probability for every possible next word.",
    "start": "6:22.48"
  },
  {
    "text": "Although researchers design the framework for how each of these steps work,",
    "start": "6:28.56"
  },
  {
    "text": "it's important to understand that the specific behavior is an emergent phenomenon",
    "start": "6:32.79"
  },
  {
    "text": "based on how those hundreds of billions of parameters are tuned during training.",
    "start": "6:37.36"
  },
  {
    "text": "This makes it incredibly challenging to determine",
    "start": "6:42.48"
  },
  {
    "text": "why the model makes the exact predictions that it does.",
    "start": "6:45.07"
  },
  {
    "text": "What you can see is that when you use large language model predictions to autocomplete",
    "start": "6:48.44"
  },
  {
    "text": "a prompt, the words that it generates are uncannily fluent, fascinating, and even useful.",
    "start": "6:53.78"
  },
  {
    "text": "If you're a new viewer and you're curious about more details on how",
    "start": "7:05.72"
  },
  {
    "text": "transformers and attention work, boy do I have some material for you.",
    "start": "7:08.83"
  },
  {
    "text": "One option is to jump into a series I made about deep learning,",
    "start": "7:12.40"
  },
  {
    "text": "where we visualize and motivate the details of attention and all the other steps",
    "start": "7:16.08"
  },
  {
    "text": "in a transformer.",
    "start": "7:20.74"
  },
  {
    "text": "Also, on my second channel I just posted a talk I gave a couple",
    "start": "7:22.10"
  },
  {
    "text": "months ago about this topic for the company TNG in Munich.",
    "start": "7:25.53"
  },
  {
    "text": "Sometimes I actually prefer the content I make as a casual talk rather than a produced",
    "start": "7:29.08"
  },
  {
    "text": "video, but I leave it up to you which one of these feels like the better follow-on.",
    "start": "7:33.10"
  }
]