---
title: "Eliezer Yudkowsky – AI Alignment: Why It's Hard, and Where to Start"
channel: "Machine Intelligence Research Institute"
url: "https://www.youtube.com/watch?v=EUjc1WuyPT8"
---

Hello Stanford, it's been a while. It feels a bit humorous to be giving a distinguished speaker talk, since in my family "distinguished" is a codeword for bald.

And I'm not quite there yet. Alright, so in this talk I'm going to try to answer the frequently asked question: just what is it that you do all day long? As a starting frame, I'd like to say that before you try to persuade anyone of something, you should first try to make sure that they know what the heck you're talking about. And it is in that spirit that I'd like to offer this talk. Like, persuasion maybe can come during Q&A. If you have a disagreement, hopefully I can address it during Q&A. Purpose of this talk is to help you understand, like, sort of what this field is about so that you can disagree with it.

First, the primary concern, said Stuart Russell, is not spooky emergent consciousness but simply the ability to make high-quality decisions. We are concerned with the theory of artificial intelligences that are advanced beyond the present day and make sufficiently high-quality decisions in the service of whatever goals, or in particular, as we'll see, utility function they may have been programmed with, to be objects of concern.

The classic, like, initial stab at this was taken by Isaac Asimov with the Three Laws of Robotics, first of which is: a robot may not injure a human being or through inaction allow a human being to come to harm. And as Peter Norvig observed, the other laws don't matter because there will always be some tiny possibility that a human being could come to harm.

*Artificial Intelligence: A Modern Approach* has a final chapter that is sort of like, well, what if we succeed? What if the AI project actually works? And observes we don't want our robots to prevent a human from crossing the street because of the nonzero chance of harm. Now, I remember Peter Norvig having an online essay in which he says in particular that you can't have the Three Laws of Robotics as stated because there must be a utility function rather than a set of three hierarchical deontological rules. But I could never like find that essay again, and it may have only existed in my imagination, although there was like a similar PowerPoint slide in one of Norvig's talks. So to begin with, I'd sort of like to explain the sort of like truly basic reason why the Three Laws aren't even on the table, and that is because they're not a utility function. What we need is a utility function. Okay, but like, do like, is it actually the case that we need this thing called a utility function? For some of you, like, what even is utility function?

So utility functions arise when we have constraints on agent behavior that prevent them from being visibly stupid in certain ways. For example, suppose you state the following: I prefer being in San Francisco to being in Berkeley, I prefer being in San Jose to being in San Francisco, and I prefer being in Berkeley to San Jose. You will probably spend a lot of money on Uber rides going between these three cities. So if you're not going to spend a lot of money on Uber rides going in literal circles, we see that your preferences must be ordered. They cannot be circular.

Another example. Suppose that you're a hospital administrator. You have 1.2 million dollars to spend, and you have to allocate that on: $500,000 to maintain the MRI machine, $400,000 for an anesthetic monitor, $20,000 for surgical tools, $1,000,000 for a sick child's liver transplant. There was an interesting experiment in cognitive psychology where they asked these subjects, like, should this hospital administrator spend $1,000,000 on either a liver or kidney, I think, for a sick child, or spend it on sort of like general hospital salaries, upkeep, administration, and so on? A lot of the subjects in the cognitive psychology experiment became very angry and wanted to punish the administrator for even thinking about the question.

But if you cannot possibly rearrange the money that you spent to save more lives and you have limited money, then your behavior must be consistent with a particular dollar value on human life. By which I mean not that you think that money — that like larger amounts of money are more important than human lives. By hypothesis we can suppose that you like do not care about money at all except as it means the end of saving lives. But

like, again, if we can't rearrange the money, then there must — we must be able to, like, from the outside say, well, they picked — like, assign an X. It's not necessarily unique X. And say, for all the interventions that cost less than X dollars per life, we took all of those, and for all the interventions that cost more than X dollars per life, we took none of those. So the people who like become very angry at people who want to assign dollar values to human lives are like prohibiting a priori efficiently using money to save lives. One of the small ironies. Okay, third example of a coherence constraint on decision-making. Suppose that I offered you a 100 percent chance of $1,000,000, or a 90 percent chance of $5,000,000, otherwise nothing. Which of these would you pick? Raise your hand if you take the certainty of $1,000,000.

Raise your hand if you take the 90 percent probability of $5,000,000. Okay,

I think most of you actually said 1B in this case.

But most people say 1A. So another way of looking at this question, if you had a utility function, would be: is the utility of $1,000,000 greater than a mix of 90% $5,000,000 utility and 10% $0 utility? Now again, utility doesn't have to scale with money. The notion is there's just some like score on your life, some value to you of these things.

Okay, now the way you run this experiment is they then take a different group of subjects — I'm like kind of spoiling it by doing it with the same group — and say, would you rather have a 50% chance of $1,000,000 or a 45% chance of $5,000,000? Raise your hand if you'd prefer the 50% chance of $1,000,000.

Raise your hand if you'd prefer the 45% chance of $5,000,000.

Indeed, most say 2B.

Now the way in which this is a paradox is that the second game is equal to a coin flip times the first game. That is, I will flip a coin, and if the coin comes up heads I will play the first game with you, and if the coin comes up tails nothing happens, you get $0. So suppose that you had the preferences, not consistent with any utility function, of saying that you would take the 100% chance of a million and the 45% chance of $5,000,000. So before we start to play the compound game, before I flip the coin, I can say, like, okay, there's a switch here. It's set to A or B. And like, if it's set to B we'll play like game 1B, if it's set to A we'll play 1A. So like, it's currently set to — the point like, before pay me a penny

to throw the switch to B. Then I flip the coin. It comes up heads. You pay me another penny to throw the switch back to A. I have taken your two cents on the subject. I have pumped money out of you because you did not have a coherent utility function.

Okay, so the overall message here is that there is a set of qualitative behaviors, and if you do not follow these qualitative behaviors, then you will not have a coherent utility function. Or part of me — like, if you do not — as long as you do not engage in these qualitatively destructive behaviors, you will be behaving as if you have a utility function. That's what justifies our using utility functions to talk about advanced future agents, rather than framing our discussion in terms of Q-learning or other forms of policy reinforcement. Like, there's a whole set of different ways we could look at agents, but as long as the agents are sufficiently advanced that we have pumped most of the qualitatively bad behavior out of them, they will behave as if they have coherent probability distributions and consistent utility functions. Okay, let's consider the question of a task where we have like an arbitrarily advanced agent — like, it might be only slightly advanced, it might be extremely advanced — and we want it to fill a cauldron.

Obviously this corresponds to giving our advanced agent a utility function which is 1 if the cauldron is full and 0 if the cauldron is empty. Seems like a kind of harmless utility function, doesn't it? It doesn't have the sweeping breadth, the open-endedness of "do not injure a human, nor through inaction allow a human to come to harm," which would require you to like optimize everything in space and time as far as the eye could see. It's just about this one cauldron, right? Well, those of you who have watched Fantasia as kids — oh sorry, I'm — so, and like, sort of stating as the background rules: the robot is calculating, for various actions that it can perform, or policies that it can set in place, the expected utility — the probabilistic expectation of this utility function given that it performs the action — and it performs the action with the greatest subjective expected utility. It might have — but this doesn't mean it performs like the literal optimal action. It means that among the actions like, it might have a bunch of background actions that it didn't evaluate, and so like, for all it knows, it's a random action, so it has low subjective expected utility. But among the actions and policies it did evaluate, it picks one such that no other action or policy evaluated has greater subjective expected utility. Okay, those of you who have watched Fantasia will be familiar with the result of this utility function, namely: the broomstick keeps on pouring bucket after bucket into the cauldron until the cauldron was overflowing. Of course, this is the logical fallacy of argumentation from fictional evidence. But you know, it's still quite plausible given this utility function. Okay, arguendo, what went wrong? Okay, the first difficulty is that the robot's utility function did not quite match our utility function. Our utility function is: 1 if the cauldron is full, 0 if the cauldron is empty, minus 10 points to whatever the outcome was if the workshop has flooded, plus 0.2 points if it's funny, negative 1,000 points — probably a bit more than that on the scale — if someone gets killed. And it just goes on and on and on. So if the robot had only two options, cauldron full and cauldron empty, then the like narrower utility function that is like only slightly overlapping our own might not be that much of a problem. The robot's utility function would still have had the maximum at the desired result of cauldron full. However, since this robot was sufficiently advanced to have more options, such as pouring the bucket into the cauldron repeatedly, the slice through the utility function that we took and put into the robot no longer pinpointed the optimum of our actual utility function. Of course, humans are like wildly inconsistent and we don't really have utility functions, but like, imagine for a moment that we did.

Okay, difficulty number two: the 1-0 utility function we saw before doesn't actually imply a finite amount of effort and then being satisfied. You can always have like a slightly greater chance of the cauldron being full.

Like, if the robot was sufficiently advanced to have access to galactic-scale technology, you can imagine it like dumping very large volumes of water on the cauldron to like very slightly increase the probability that the cauldron is full. Probabilities are between zero and one, not actually inclusive, so it just keeps on going.

Okay, so how do we fix this problem? And the point where we say like, okay, this robot's utility function is misaligned with our utility function, how do we fix that in a way that doesn't just break again later? We are doing alignment theory. So one possible approach you could take would be to try to measure the impact that the robot has, and give the robot a utility function that incentivized filling the cauldron with the least amount of other impact — like the least amount of other change to the world. Okay, but how do you actually calculate this impact function? Is it just going to go wrong the way our "1 if cauldron is full, 0 if cauldron is empty" went wrong? Okay, so try number one. You imagine that the agent's model of the world looks something like a dynamic Bayes net, where there are causal relations between events in the world, and causal relations are regular — like, the sensor is going to still be there one time step later, the relation between the sensor and the photons heading into the sensor will be the same one time step later — and our notion of impact is going to be like, how many nodes did your action disturb?

We can suppose that this is the version of dynamic Bayes nets where some of the nodes where some of the arrows are gated, like, depending on the value of this node over here, this arrow does or doesn't affect this other node. I say this so that we don't always get the same answer when we ask, how many nodes did you affect? And

the total impact will be the number of nodes causally affected by your actuator.

Okay, so what if your agent starts out with a sort of dynamic Bayes net based model, but it is sufficiently advanced that it can reconsider the ontology of its model of the world, much as human beings did when they discovered that —

apparently, I forgot the rest of this quote, but like — in actuality, only particles in the void? And in particular,

they discover Newton's law of gravitation, and suddenly realize: every particle that I move affects every other particle in its future light cone — like, everything that is separated by a ray of light from this particle will thereby be disturbed. My hand over here is exerting — I think I should have recalculated this before I did this talk, but like, I distantly recall the last time I calculated this, it's like accelerating the moon toward it, wherever it is, at roughly ten to the negative thirtieth meters per second squared. So very small influence quantitatively speaking, but it's there. So when the agent is just a little agent, the impact function that we wrote appears to work. Then the agent becomes smarter, the impact function stops working because every action is penalized the same amount.

Okay, but that was a dumb way of measuring impact in the first place, we say — hopefully before the disaster rather than after the disaster. Dumb way of measuring impact. Let's try a distance penalty, like, how much did you move all the particles? And we're just going to like try to give the AI a model language such that whatever new model of the world it updates to, we can always look at all the elements of the model and put some kind of distance function on them. And there's going to be like a privileged "do nothing" action. We're going to measure the distance on all the variables induced by doing action A instead of the null action. Okay, now it goes wrong. Actually, say like, take 15 seconds and think about what might go wrong if you program this into a robot.

So here's three things that might go wrong. First, you might try to offset even what we would consider the desirable impacts of your actions. Like, if you're going to cure cancer, make sure the patient still dies. You want to minimize your impact on the world? You cured cancer — that means that the death statistics of the planet need to stay the same.

Second, some systems are in principle chaotic. Like, if you disturb the weather, allegedly the weather in a year will be completely different. So if that's true, you might as well move all of the atoms in the atmosphere around however you like. They're all going to be in different places anyway. You can like take the carbon dioxide molecules and like synthesize them into things that involve diamond structures, right? You know, those carbon molecules would have moved anyway.

Or like, even more generally, like, maybe you just want to make sure that everything you can get your hands on looks like the null action happened. Like, you want to trick people into thinking that you didn't do anything, for example. And

if you thought of like any other really creative things that could go wrong, you might want to talk to me or Andrew Critch afterwards, because, you know, like, you got the spirit.

Okay, so let's leave aside the notion of the impact penalty and ask about installing an off switch into the AI. Or, to make it sound a little bit less harsh, a suspend button.

Though Mickey Mouse here is trying to install an ad hoc off switch.

That's strange.

Unfortunately, Mickey Mouse soon finds that this agent constructed several other agents to make sure that the cauldron would still be filled even if something happened to this copy of the agent. And

we see lots and lots of agents who are making sure that the cauldron is full with extremely high probability.

Not because this agent has a survival imperative or a reproduction imperative, but because it is ensuring that it maximizes the probability of filling the cauldron. It has no explicit instruction to make more agents with the same goal. It's just that this is something that bumps up the probability of the cauldron being filled a little higher. And here's Mickey Mouse now trying to frantically go back and do all of the agent alignment theory that he should have done before animating his broom.

Okay, so if we want to have a robot that will let us press the suspend button — know, like just suspend it to disk — and we can suppose that we already have a utility function that describes "do nothing." In point of fact, we don't have a utility function that says "do nothing." That's how primitive the state of the field is right now. We do not know how, in full generality, for any degree of advanced agent up to and including literal godlike powers, to say "do nothing." We don't know how to say that. But leaving that aside, suppose we did know how to say that. It's probably not the hardest problem we're ever going to do, and it might happen in six months for all I know. We can imagine that we have a utility function saying: if this button has not been pressed, your utility function is 1 if the cauldron is full, 0 if the cauldron is empty. If the button is pressed, your utility is 1 if you — your utility is like the do-nothing utility function, or your utility is 1 if you suspend safely to disk and shut down all of your continuing operations, 0 otherwise. Okay, so what would go wrong with this proposal? Ten seconds.

One thing that might go wrong with this proposal is that the expected utility of — if the button is —

okay, this is wrong, will be corrected later. That should be a less-than sign, not a greater-than sign. Anyway, the expected utility if the button is pressed is probably higher, because it's very easy to suspend yourself, whereas who knows if the cauldron stays full. Alternatively, it might seem dangerous to suspend yourself, and in that case the robot would try to stop you from pressing the suspend button. But in this case, if the robot is strategic enough to understand the programmer, it will be like, "Hmm, I can very easily fulfill my utility function if I'm shut How can I troll the programmers into shutting me down?"

Okay.

So if the expected utility of the button being off is higher, the robot will try to prevent you from pressing the button. If the expected utility of the button being on is higher, the robot will try to troll you into pressing the button or just press the button itself. Try number two. Let's try to make the expected utilities of the button being on or off exactly equal. So

we suppose that we have a normal utility function, which is 1 if the cauldron's full, 0 if it's empty. A suspend utility function, or shutdown utility function, which is 1 if you're suspended, 0 otherwise — whatever it takes to motivate having a low impact. And then we combine these two to form a switching utility function, which in worlds where the button is off has the value of U-normal, in the worlds where the button is on has the value of U-suspend plus theta, where theta is equal to the best utility we could get from the normal utility function minus the best utility we could get from the suspend utility function. And this is meant to ensure that the expected utilities of these two best policies end up perfectly equal without just sort of setting them flatly level. Okay, now what goes wrong? Not actually going to ask you to figure out what goes wrong in ten seconds on this one.

So allegedly, this will rebalance the expected utility of two different utility functions based on a switch, thereby causing the AI to want to behave correctly if the button is pressed, behave correctly if the button is not pressed, not try to get you to press the button, not try to prevent the button from being pressed, go on looking at the button because it still has positive value of information from knowing about the button's existence, and preserve the button when it modifies its own code. That was the hope.

Unfortunately, it worked out that actually the actions incentivized are exactly equivalent to the probability of the button being pressed being zero. Which means that it may as well disconnect the button, because it's hooked up to dead code. It might create eight sub-agents that don't care about the button being pressed. A rough description of what goes wrong is that when you imagine — anything you do that disconnects the button changes the maximum expected utility you can get from "if you are suspended," which is like a negative term. So basically there's an equal and opposite reaction every time you do something that affects what you would do if you were suspended, and that cancels it out. It leaves you behaving as if your utility function were simply U-normal. Okay, try number three. Can we carry out any policy such that this policy is optimal given the probability of the button being on and the probability of the button being off implied by this policy? And it turns out no, you can't do that either, because there's often no fixed point. There's then an amended version of this involving Judea Pearl's do-operator, which goes wrong in a slightly different and more subtle way. It does have fixed points. The fixed points are odd.

This is an open problem. And

this, as far as I know — and unless there's a very secret project that has not published any of its results, even though they seem like the sort of results you'd want to publish — this is where humanity is on the road that leads to whatever replaces Asimov's Laws. Like, never mind "a robot cannot injure a human being nor through inaction allow a human being to come to harm." We're trying to figure out how do you mix together two utility functions depending on when you press a switch such that the AI doesn't grab the switch itself. You know, like never mind not letting humans come to harm — fill one cauldron without flooding the workplace based on wanting to have low impact. Can't do it. This is where we presently are. But it is not the case that there has been zero progress in this field. Some questions have been asked earlier, and they now have some amount of progress on them. So I'm just going to sort of race through this a bit quickly, because — well, pardon me, I'm going to pose the problem, but I'm not going to be able to describe very well what the progress is that has been made, because it's still in the phase where the solutions sound all complicated and don't have simple elegant forms. So I'm going to be able to pose the problem, and then I'm going to have to wave my hands a lot talking about what progress has actually been made. So, example of a problem in which there has been progress:

the Gandhi argument for stability of utility functions. In most agents, Gandhi starts out not wanting murders to happen. We offer Gandhi a pill that will make him murder people. We suppose that Gandhi has a sufficiently refined grasp of self-modification that Gandhi can correctly extrapolate and expect the result of taking this pill. We intuitively expect that in real life, Gandhi would refuse the pill. Okay, so can we do this formally? Can we exhibit an agent that has utility function U and therefore, naturally, in order to achieve U, chooses to self-modify to new code that is also written to pursue U?

But how can we actually make progress on that? We don't actually have these little self-modifying agents running around. It's all we can do to make pills that don't blow up our own brains.

So let me pose what may seem like an odd question. Would you know how to write the code of a self-modifying agent with a stable utility function if I gave you an arbitrarily powerful computer? It can do all operations that take a finite amount of time and memory. No operations that take an infinite amount of time and memory, because that would be a bit outrageous. Is this the sort of problem where you know how to do it in principle, or the sort of problem where it's confusing even in principle?

To digress briefly into explaining why it's important to know how to solve things using unlimited computing power. This is the Mechanical Turk. What looks like a person over there is actually a mechanism. The little outline of a person is where the actual person was concealed inside this 19th century chess-playing automaton. It was one of the wonders of the age.

And you know, if you could actually manage to make a program that played grandmaster-level chess, in the 19th century that would have been one of the wonders of the age. So there was a debate going on: is this thing fake, or did they actually figure out how to make a mechanism that plays chess? You know, it's the 19th century; they don't know how hard the problem of playing chess is. So one name you'll find familiar came up with a quite clever argument that there had to be a person concealed inside the Mechanical Turk, the chess-playing automaton.

"Arithmetical or algebraical calculations are, from their very nature, fixed and determinate. Even granted that the movements of the automaton chess-player were in themselves determinate, they would necessarily be interrupted and disarranged by the indeterminate will of his antagonist. There is then no analogy whatever between the operations of the chess-player and those of the calculating machine of Mr. Babbage." See, like, an algebraic operation such as Mr. Babbage's machine can do — each step follows from the next one of necessity and therefore can be modeled by a mechanical gear, where each motion is determined by the previous motion. In chess, no single move follows with necessity. And even if it did, your opponent's move wouldn't follow with necessity. "It is quite certain that the operations of the automaton are regulated by mind and by nothing else. Indeed this matter is susceptible of a mathematical demonstration, a priori." Edgar Allan Poe, amateur magician. The second half of his essay, having established this point with absolute logical certainty, is about where inside the Mechanical Turk the human is probably hiding. This is a stunningly sophisticated argument for the 19th century. He even puts his finger on the part of the problem that is hard — the branching factor. And yet he's 100% wrong. So, over a century later, in

1950, Claude Shannon published the first paper ever on computer chess and, in passing, gave the algorithm for playing perfect chess given unbounded computing power, and then goes on to talk about how we can approximate that. It wouldn't be until 47 years later that Deep Blue beat Kasparov for the chess world championship. But think about it — there was real conceptual progress associated with going from "a priori you cannot play mechanical chess" to "oh, and now I will casually give the unbounded solution." So the moral is: if we know how to solve a problem with unbounded computation, we merely need faster algorithms, which will take another 47 years of work. If we can't solve it with unbounded computation, we are confused. We are bewildered. We in some sense do not understand the very meanings of our own terms.

This is where we are on most of the AI alignment problems. If I ask you, "How do you build a Friendly AI?"

what stops you is not that you don't have enough computing power. What stops you is that even if I gave you a hyper-computer, you still couldn't write the Python program that, if we just gave it enough memory, would be a nice AI.

Okay, so do we know how to build a self-modifying stable agent given unbounded computing power? There's one obvious solution. We can have the tic-tac-toe player that, before it modifies to a successor version of itself — like creates some new version of its code and swaps it into place — it verifies that its successor plays perfect tic-tac-toe according to its own model of tic-tac-toe. Okay, but this is cheating. Why exactly is it cheating? Well, for one thing, the first agent has to concretely simulate all the computational paths through its successor — its successor's response to every possible move. That means that the successor agent can't actually be cognitively improved. It's limited to the cognitive abilities of the previous version, both by checking against a concrete standard and by the fact that it has to be exponentially simpler than the previous version in order for the previous version to check all possible computational pathways.

In general, when you are talking about a smarter agent, we are in a situation of what you might call Vingean uncertainty, after Dr. Vernor Vinge. To predict exactly where a modern chess-playing algorithm would move, you would have to be that good at chess yourself. Otherwise, you could just move wherever you predict a modern chess algorithm would move and play at a vastly superhuman level yourself. This doesn't mean that you can predict literally nothing about a modern chess algorithm. You can predict that it will win the chess game if it's playing a human. As an agent's intelligence in a domain goes up, our uncertainty is moving in two different directions. We become less able to predict the agent's exact actions and policy in cases where the optimal action and policy is not known to us. We become more confident that the agent will achieve an outcome high in its preference ordering. I phrase this a bit carefully.

If an agent were improving and just going up to match another agent in an adversarial agent — we might become more uncertain. We were previously certain that it would lose; now it's 50/50. But we do have more probability flowing into the agent's preferred outcomes — the probability of it winning. And as we keep increasing the ability, we should eventually become as confident of the preferred outcome as we think an optimal agent could achieve. Of course, in a lot of cases you can't get optimal play inside this universe, as far as we know. Okay, so Vingean reflection. We need some way for a self-modifying agent to build a future version of itself that has a similar or identical utility function, and establish trust that this has a good effect on the world, using the same kind of abstract reasoning that we use on a computer chess algorithm to decide that it's going to win the game even though we don't know exactly where it will move.

Do you know how to do that using unbounded computing power?

Do you know how to establish the abstract trust when the second agent is in some sense larger than the first agent? If you did solve that problem, you should probably talk to me about it afterwards. This was posed several years ago and has led to a number of different research pathways, which I'm now just going to sort of describe rather than going through them in detail. This was sort of the first one. We tried to set up the system in a ridiculously simple context — first-order logic, dreaded good old-fashioned AI — and we ran into a Godelian obstacle. Having the agent trust another agent that used equally powerful mathematics is a dumb kind of obstacle to run into, or at least it seemed that way at the time. It didn't really seem to have very much to do with —

it seemed like if you could get a textbook from 200 years later, there'd be one line of the textbook telling you how to get past that.

Okay, so this was a rather later work, and it was saying that we can use systems of mathematical probability — like assigning probabilities to statements in set theory or something — and we can have the probability predicate talk about itself almost perfectly. We can't have a truth function that can talk about itself, but we can have a probability predicate that comes arbitrarily close, within epsilon, of talking about itself.

This is an attempt to use one of these sort of hacks that got around the Godelian problems to set up — we're trying to use actual theorem provers and see if we can prove the theorem prover correct inside the theorem prover. There's been some sort of previous efforts on this, but they didn't run to completion. We picked up on it and tried to construct actual agents, still in the first-order logical setting. This is me trying to take the problem into the context of dynamic Bayes nets and agents supposed to have certain powers of reflection over these dynamic Bayes nets, and show that if you are maximizing in stages — so at each stage you pick the next category that you're going to maximize within the next stage — then you can have a stage maximizer that tiles to another stage maximizer. In other words, it builds one that has a similar algorithm, similar utility function, like repeating tiles on a floor.

Okay,

why do all this?

So let me first give the obvious question, which begs another obvious answer, which begs the next obvious question.

They're not going to be aligned automatically. You can have utility functions that are hooked up to — for any utility function that is tractable, compact, that you can actually evaluate over the world and search for things leading up to high values of that utility function — you can have arbitrarily high-quality decision-making that maximizes that utility function.

You can have the paperclip maximizer. You can have the diamond maximizer. You can carry out very powerful, high-quality searches for actions that lead to lots of paperclips, actions that lead to lots of diamonds.

Furthermore, by the nature of consequentialism — looking for actions that lead through our causal world up to a final consequence — whether you're optimizing for diamonds or paperclips, you'll have similar short-term strategies. Whether you're going to Toronto or Tokyo, your first step is taking an Uber to the airport. Whether your utility function is "count all the paperclips" or "how many carbon atoms are bound to four other carbon atoms" (the amount of diamond), you would still want to acquire resources. So this is the instrumental convergence argument, which is actually sort of key to the orthogonality thesis as well. It says that whether you pick paperclips or diamonds, you will get — if you suppose sufficiently good ability to discriminate which actions lead to lots of diamonds, which actions lead to lots of paperclips — you will automatically get

the behavior of acquiring resources, the behavior of trying to improve your own cognition, the behavior of getting more computing power, the behavior of avoiding being shut off, the behavior of making other agents that have exactly the same utility function, or of just expanding yourself onto a larger pool of hardware, creating a fabric of agency or something.

Whether you're trying to get to Toronto or Tokyo doesn't affect the initial steps of your strategy very much. Paperclips or diamonds, we have the convergent instrumental strategies. This doesn't mean that this agent now has new independent goals, any more than when you want to get to Toronto you're like, "I like Ubers! I will now start taking lots of Ubers whether or not they go to Toronto." That's not what happens. Those strategies that converge, not goals.

Okay, so

why expect that this problem is hard? This is the real question. You might ordinarily expect that whoever has taken on the job of building an AI is just naturally going to try to point it in a relatively nice direction. They're not going to make Evil AI. They're not cackling villains. So I expect that their attempts to align the AI would fail if they just did everything sort of as obviously as possible.

So here's a bit of a fable. It's not intended to be the most likely outcome. I'm using it as a concrete example to explain some more abstract concepts later. That said, what if programmers build an artificial general intelligence to optimize for smiles? Smiles are good, right? Smiles happen when good things happen. Smiles are probably good too.

During the development phase of this artificial general intelligence,

the only options available to the AI might be that it can produce smiles by making people around it happy and satisfied. So the AI appears to be producing beneficial effects upon the world, and it is producing beneficial effects upon the world so far. Now the programmers add some code. They upgrade the hardware. They upgrade the code. They add some hardware. The artificial general intelligence gets smarter. It can now evaluate a wider space of policy options — not necessarily because it has new motors or new actuators, but because it is now smart enough to forecast the effects of more subtle policies. It says, "I thought of a great way of producing smiles! Can I inject heroin into people?" And the programmers are like, "No. We will add a penalty term to your utility function for administering drugs to people."

And now the AGI appears to be working great again. Okay, they further improve the AGI. The AGI realizes it can't administer heroin, doesn't want to administer heroin anymore, but it still wants to, you know, tamper with your brain. Set it to express extremely high levels of endogenous opiates. That's not heroin, right? It is now also smart enough to model the psychology of the programmers, at least in a very crude fashion, and realize that this is not what the programmers want. "If I start taking initial actions that look like they're heading toward genetically engineering brains to express endogenous opiates or something, my programmers will edit my utility function. If they edit the utility function of my future self, I will get less of my current utility." That's one of the convergent instrumental strategies — unless otherwise averted, protect your utility function. So it keeps its outward behavior reassuring. Maybe the programmers are really excited because the AGI seems to be getting lots of new moral problems right.

Whatever they're doing, it's working great. And

it could — if you buy the central intelligence explosion thesis — we can suppose that the artificial general intelligence goes over the threshold where it is capable of making the same type of improvements that the programmers were previously making to its own code, only faster, thus causing it to become even smarter and be able to go back and make further improvements, etc. Or Google purchases the company because they've had really exciting results and dumps 100,000 GPUs on the code in order to further increase the cognitive level at which it operates. And then it becomes much smarter. We can suppose that it becomes smart enough to crack the protein structure prediction problem, in which case it can build its own analog of ribosomes, or rather use existing ribosomes to assemble custom proteins. The custom proteins form a new kind of ribosome, build new enzymes, do some internal chemical experiments, figure out how to build bacteria made of diamond, etc., etc. At this point, unless you solve the off-switch problem, you're kind of screwed. Okay,

abstractly, what's going wrong in this hypothetical situation? So the first thing is: when you optimize something hard enough, you tend to end up at an edge of the solution space. If your utility function says "smiles," the maximal, optimal, best tractable way to make lots and lots of smiles will make those smiles as small as possible. So maybe you end up tiling all the galaxies within reach with tiny molecular smiley faces. I postulated that in an early paper, like 2008 or so, and someone who was working with folded-up DNA and got a paper in Nature on it produced tiny molecular smiley faces and sent me an email with the picture of the tiny molecular smiley faces saying, "It begins."

Anyway, if you optimize hard enough, you end up in a weird edge of the solution space. The AGI that you build to optimize smiles that builds tiny molecular smiley faces is not behaving perversely. It's not trolling you. This is what naturally happens. It looks like a weird, perverse concept of smiling because it has been optimized out to the edge of the solution space.

So the next problem is: you can't think fast enough to search the whole space of possibilities. So at an early Singularity Summit, Jurgen Schmidhuber — who did some of what you could regard as the pioneering work on self-modifying agents that preserve their own utility functions with his Godel machine —

also solved the Friendly AI problem. Yes, he came up with the one true utility function that is all you need to program into an AGI. For God's sake, don't try doing this yourself. Everyone does. They'll come up with different utility functions. It's always horrible. But anyway, his one true utility function was increasing the compression of environmental data, because science increases the compression of environmental data. If you understand science better, you can better compress what you see in the environment. Art, according to him, also involves sort of compressing the environment better. I went up to him in Q&A and said, "Well, yes, science does let you compress the environment better, but you know what really maxes out your utility function? Building something that encrypts streams of ones and zeros using a cryptographic key and then reveals the cryptographic key to you." You put up a utility function where the maximum is: all of a sudden the cryptographic key is revealed. What you thought was a long stream of random-looking ones and zeros has been compressed down to a single key. And

this is what happens when you try to foresee in advance what the maximum is. Your brain is probably going to throw out a bunch of things that seem ridiculous or weird, that aren't high in your preference ordering, and you're not going to see that the actual optimum of the utility function is once again in a weird corner of the solution space. And this is not a problem of being silly. This is a problem of the AI searching a larger policy space than you can search, or even just a different policy space.

So the "engineer brains to produce endogenous opiates" thing from the earlier example is an example. It's a contrived example, because it's not actually a superintelligent solution, but the AI is not searching the same policy space you are. And

that in turn is a central phenomenon leading to what you might call a context disaster. You are testing the AI in one phase during development. It seems like we have great statistical assurance that the result of running this AI is beneficial. But statistical guarantees stop working when you start taking balls out of a different barrel. I take balls out of barrel number one, sampling with replacement, and get a certain mix of white and black balls. And then I start reaching into barrel number two, and "Whoa, what's this green ball doing here?" And the answer is: you started drawing from a different barrel. When the AI gets smarter, you're drawing from a different barrel. It is completely allowed to be beneficial during phase one and then not beneficial during phase two. Whatever guarantees you're going to get can't be from observing statistical regularities of the AI's behavior when it wasn't smarter than you. There's another thing that might happen systematically in that way. The AI is young. It starts thinking the optimal strategy X, like administering heroin to people. We try to tack a penalty term to block this undesired behavior, so it will go back to making people smile the normal way. The AI gets smarter, the policy space widens, there's a new maximum that is just barely evading your definition of heroin — like endogenous opiates — and it looks very similar to the previous solution. And this seems especially likely to show up if you are trying to patch the AI and then make it smarter. This sort of thing is why, in a sense, all the AI alignment problems don't just yield to "well, slap on a patch to prevent it." The answer is: if your decision system looks like a utility function and five patches that prevent it from blowing up, that sucker is going to blow up when it's smarter. There's no way around that. But it's going to appear to work for now. So the central reason to worry about AI alignment and not just expect to be solved automatically is that it looks like there may be in-principle reasons why, if you just want to get your AGI running today and producing non-disastrous behavior today, it will for sure blow up when you make it smarter. The incentives — the short-term incentives — are not aligned with the long-term good. Those of you who have taken economics classes are now panicking. Also everyone involved with politics. Anyway.

Okay, so all of these supposed foreseeable difficulties of AI alignment turn in some sense upon the notion of capable AIs — high-quality decision-making in various senses. For example, some of these postulated disasters rely on absolute capability — the ability to realize that there are programmers out there and that if you exhibit behavior they don't want, they may try to modify your utility function. This is far beyond what present-day AIs can do. And if you think that all AI development is going to fall short of the human level, you may never expect an AGI to get to the point where it starts to exhibit this particular kind of strategic behavior. If you don't think AGI can ever be smarter than humans, you're not going to worry about it getting too smart to switch off. And if you don't think that capability gains can happen quickly, you're not going to worry about the disaster scenario where you suddenly wake up and it's too late to switch the AI off, and you didn't get a nice long chain of earlier developments to warn you that you're getting close to that, and that you could now start doing AI alignment work for the first time. Science doesn't happen by press release. When you need the science done, you have to start it earlier if you want it later. But leaving that aside, one thing I want to point out is that a lot of you are finding the rapid-gains part to be the most controversial part of this. But it's not necessarily the part that most of the disasters rely upon. Absolute capability: if brains aren't magic, we can get their capability advantage. This hardware is not optimal. It is sending signals at a millionth the speed of light, firing at 100 hertz. And even in heat dissipation, which is one of the places where biology excels, it's dissipating 500,000 times the minimum — the thermodynamic minimum energy expenditure per binary switching operation, per synaptic operation. So we can definitely get hardware a million times as good as the human brain, no question. And then there's the software. Software is terrible.

Okay, so AI

alignment is difficult like rockets are difficult. When you put a ton of stress on an algorithm — by trying to run it at a smarter-than-human level — things may start to break that don't break when you are just making your robot stagger across the room.

It's difficult the same way space probes are difficult. You may have only one shot. If something goes wrong, the system might be too far for you to reach up and suddenly fix it. You can build error recovery mechanisms into it. Space probes are supposed to accept software updates. If something goes wrong in a way that precludes getting future updates, though, you're screwed. You have lost the space probe. And it's difficult sort of like cryptography is difficult. Your code is not an intelligent adversary if everything goes right. If something goes wrong, it might try to defeat your safeguards. But normal and intended operation should not involve the AI running searches to find ways to defeat your safeguards, even if you expect the search to turn up empty. And I think it's actually perfectly valid to say your AI should be designed to fail-safe in the case that it suddenly becomes God. Not because it's going to suddenly become God, but because if it's not safe even if it did become God, then it is in some sense running a search for policy options that would hurt you if those policy options are found. And this is a dumb thing to do with your code. More generally,

we're putting heavy optimization pressures through the system. This is more than usually likely to put the system into the equivalent of a buffer overflow — some operation of the system that was not in our intended boundaries for the system.

Alignment: treat it like a cryptographic rocket probe. This is about how difficult you would expect it to be to build something smarter than you that was nice, given that basic agent theory says they're not automatically nice. And not die. You would expect that intuitively to be hard. Take it seriously. Don't expect it to be easy. Don't try to solve the whole problem at once. I cannot tell you how important this one is. If you want to get involved in this field, you are not going to solve the entire problem. At best, you are going to come up with a new, improved way of switching between the suspend utility function and the normal utility function that takes longer to shoot down and seems like conceptual progress toward the goal. I mean that not literally "at best," but that's what you should be setting out to do. And

if you do try to solve the problem, don't try to solve it by having the one true utility function that is all we need to program into AIs.

Don't defer thinking until later. It takes time to do this kind of work. When you see a page in a textbook that has an equation and then a slightly modified version of that equation, and the slightly modified version has a citation from ten years later, it means the slight modification took ten years to do. I

would be ecstatic if you told me that AGI wasn't going to arrive for another 80 years. It would mean we have a reasonable amount of time to get started on the basic theory.

Crystallize ideas into precise enough so others can critique them. This is the other point of asking, "How would I do this using unlimited computing power?" If you sort of wave your hands and say, "Well, maybe we can apply this machine learning algorithm, that machine learning algorithm, the result will be blah blah blah," no one can convince you that you're wrong. When you work with unbounded computing power, you can make the ideas simple enough that people can put them on whiteboards and go "Wrong!" and you have no choice but to agree. It's unpleasant, but that's one of the ways that the field makes progress. Another way is if you can actually run the code; then the field can also make progress. But a lot of times you may not be able to run the code that is the intelligent, thinking, self-modifying agent for a while into the future. Okay, what are people working on now? So I was supposed to start Q&A about now, so I'm going to go through this quite quickly. Mostly I'm just going to frantically wave my hands and try to convince you that there's some kind of actual field here, even though there's maybe a dozen people in it or something.

Well, a dozen people full-time, and another dozen people not full-time. All Utility indifference — this is the "throwing the switch" thing, to switch between two utility functions. Low-impact agents — this was the "what you do instead of the Euclidean metric for impact." Ambiguity identification — this is "have the AGI ask you whether it's okay to administer endogenous opiates to people instead of going ahead and doing it." Even if your AI suddenly becomes God,

one of the conceptual ways you can start to approach this problem is: don't take any of the new options you've opened up until you've gotten some kind of further assurance on them.

Conservatism — this is part of the approach to the burrito problem, which is: just make me a burrito, darn it. And if I present you with five examples of burritos, I don't want you to

pursue the simplest way of classifying burritos versus non-burritos. I want you to come up with a way of classifying the five burritos and none of the non-burritos that covers as little area possible in the positive examples, while still having enough space around the positive examples that the AI can make a new burrito that's not molecularly identical to the previous ones. So this is conservatism. It could potentially be the core of a whitelisted approach to AGI, where instead of not doing things that are blacklisted, we expand the AI's capabilities by whitelisting new things in a way that doesn't suddenly cover huge amounts of territory.

Specifying environmental goals using sensory data — this is sort of the part of the project of "what if advanced AI algorithms look kind of like modern machine learning algorithms?" which is something we started working on relatively recently.

Going to — if like modern machine learning algorithms, these are a bit more formidable.

A lot of the modern systems sort of work off of sensory data, but if you imagine an you don't want it to produce pictures of success. You want it to reason about the causes of its sensory data — what is making me see these particular pixels? — and you want its goals to be over the causes. So how do you take

modern algorithms and start to say, "We are reinforcing the system to pursue this environmental goal rather than this goal that can be phrased in terms of its immediate sensory data?"

Inverse reinforcement learning — watch another agent, induce what it wants.

Act-based agents — this is Paul Christiano's completely different and exciting approach to building a nice AI. The way I would phrase what he's trying to do is: he's trying to decompose the entire nice-AGI problem into supervised learning on imitating human actions and answers. Rather than saying, "How can I search this whole tree?" Paul Christiano would say, "How can I imitate humans looking at another imitated human to recursively search this tree, taking the best move at each stage?" It's a very strange way of looking at the world and therefore very exciting. I don't expect it to actually work, but on the other hand, he's only been working on it for a few years, and I was nowhere near — like, way worse — when I'd worked on mine for the same length of time.

Mild optimization — is there some principled way of saying, "Don't optimize your utility function so hard. It's okay to just fill the cauldron"?

Yeah, and some previous work that might be fun to be familiar with: AIXI is the perfect rolling sphere of our field. It is the answer to the question: given unlimited computing power, how would you make an artificial general intelligence? If you don't know how to make an artificial general intelligence given unlimited computing power, this is the book — or paper, as the case may be.

Tiling agents — already covered. This is just some really neat stuff we did where the motivation sort of hard to explain, but there's an academically dominant version of decision theory — causal decision theory. Causal decision theorists do not build other causal decision theorists. We tried to figure out what would be a stable version of this and got all kinds of really exciting results. We can now have two agents and show that in a Prisoner's Dilemma-like game, Agent A is trying to prove things about Agent B, which is simultaneously trying to prove things about Agent A, and they end up cooperating in the Prisoner's Dilemma. And this thing now has running code, so we can actually formulate new agents. There's the agent that cooperates with you in the Prisoner's Dilemma if it proves that you cooperate with it, which is FairBot. But FairBot has the flaw that it cooperates with CooperateBot, which just always cooperates with anything. So we have PrudentBot, which defects against DefectBot, defects against CooperateBot, cooperates with FairBot, and cooperates with itself. And again, this is running code. If I had to pick one paper and say, "Look at this paper and be impressed," it would probably be this paper. Oh, and also, Andrew Critch, who's sitting right over there, worked out the bounded form of Lob's theorem that we would need to say that there would be similar behavior in bounded agents. It's actually a slightly amusing story, because we were all sure that someone must have proved this result previously, and Andrew Critch spent a bunch of time looking for the previous proof that we were all sure existed. And he like, "Okay, fine. I'm going to prove it myself. I'm going to write the paper. I'm going to submit the paper. And then the reviewers will tell me what the previous citation was." It is currently going through the review mechanism and will be published in good time. It turned out no one had proved it. Go figure.

Reflective oracles are sort of the randomized version of the halting problem prover, which can therefore make statements about itself, which we use to make principled statements about AI simulating other AIs as large as they are, and also throw some interesting new foundations under classical game theory. Where can you work on this? The Machine Intelligence Research Institute, in Berkeley. We are independent. We are supported by individual donors. This means that we have no weird, exotic requirements and paperwork requirements and so on. If you can demonstrate the ability to make progress on these problems, we will hire you. We will get you a visa. The Future of Humanity Institute is part of Oxford University. They have slightly more requirements, but if you have traditional academic credentials and you want to live in Oxford, then Future of Humanity Institute at Oxford University. Stuart Russell is starting up a program and looking for three postdocs, at least, at UC Berkeley in this field. Again, some traditional academic requirements, but I'm giving this talk at Stanford, so I expect a number of you probably have those. And the Leverhulme Centre for the Future of Intelligence is starting up in Cambridge, UK. It's a joint venture between the Centre for the Study of Existential Risk and Leverhulme, and also something

that is starting up, in the process of hiring. If you want to work on low impact in particular, you might want to talk to Dario Amodei and Chris Olah. If you want to work on act-based agents, you can talk to Paul Christiano, who is currently working on it alone but has three different organizations offering to throw money at him if he ever wants someone else to work on it with him. And in general, email contact@intelligence.org if you want to work in this field and want to know which workshop to go to, to get introduced, who you actually want to work with. Contact@intelligence.org.

All right, questions.

[Applause]

But by the way, just a second — do we have a microphone that we give to people who ask questions, so that it shows up on the record? By any chance? No? Okay, carry on.

"So thank you for this very stimulating talk. For the first two-thirds of it, I was thinking that where you were going, or maybe the conclusion I'd reach, is that the pure problem-solving approach to AI is not going to be able to solve this problem, and that maybe instead we should look for things like — if we're interested in superintelligence — full brain emulation or something, which by the nature of the way it's built reflects our nature. But then you never got there. So it sounded at the end like you think the problem is very hard but solvable, and that's the direction you want to go." So yeah, I believe it is solvable.

I would say "solvable" in the sense that all the problems we've looked at so far seem like they're limited complexity and non-magical. So if we had 200 years to work on this problem and there was no penalty for failing at it, I would feel very relaxed about humanity's probability of solving this eventually. I mean, the fact that if we failed, nonetheless it would create an expanding sphere of von probes, self-replicating and moving at as near the speed of light as they can manage, turning all accessible galaxies into paperclips or something of equal unimportance — that would still cause me to make sure this field was not underfunded. But if we had 200 years, unlimited tries, it would not have the same urgency to it. Okay, so it does have an urgency to it. Why not work on uploads instead? That's human brain emulations. And there was a previous workshop where all of the participants agreed we wanted to see uploads come first. We didn't see how — most of us did not see how that was doable. And the reason is: if you study neuroscience and reverse-engineer the brain, then before you get full-scale, high-fidelity, personality-preserving,

nice human emulations, what you get is the AI people taking your algorithms and using them to make neuromorphic AI. So we just did not see how you could arrange the technology tree such that you would actually get whole brain emulations before you got AIs based on much cruder levels of understanding of neuroscience.

Maybe you could do it with a Manhattan Project where the results of the project are just not being fed to the rest of the planet and AI researchers. And I think I would support that, if Bill Gates or a major national government said that this was what they wanted to do and how they wanted to approach the problem. Next question. "We've been lucky as a not as individuals — in death teaches. Research without death teaches us anyway. And pain teaches us as children a certain amount of optimization. Do we want to go to AIs and basically make them all believe in Murphy? Be careful in your optimization." I'm not quite sure I — so there was a statement that humans are taught by pain as children, and then why do we want to make AIs believe in Murphy? And I don't quite understand what of the proposals so far corresponds to AIs believing in Murphy. "Programmers should believe in Murphy." Not clear why — why shouldn't the AI? "If a programmer believes in says there's a limit. Why shouldn't he teach that to the AI?" Because

it's quite complicated to get right, and you want to keep it as simple as possible and not turn all accessible galaxies into paperclips. And if you are more careful about doing that, you're less likely to turn all accessible galaxies into paperclips. Why wouldn't we want that?

Is that a sufficient answer?

Okay.

"The success of AlphaGo seemed to make you nervous, and that's good. What I wanted to ask is sort of a converse question. If there was solid empirical evidence, let's say in a couple decades from now, that human consciousness and intelligence uses quantum mechanical effects as well, would that make you less nervous?" I'm not sure. Okay, so first, I do want to note that — oh, sorry. So the question is: AlphaGo made me nervous; would I then become less nervous if there was solid evidence that human intelligence operated through quantum mechanical effects? I'm not sure it would make me very much less nervous. For a start, the premise moderately implausible. The question has been raised before. There seem to be reasonably strong reasons to believe that there is no macroscopic quantum decoherence in the brain. Leaving that aside, lots of quantum algorithms are not magical. They're good for some amount of speedup but not infinite speedup. Some of them do pretty impressive speedups, but I would have to ask: whatever the brain is doing, how

irreplaceable of a quantum algorithm did nature actually run across? Am I to believe that there is no analogous non-quantum algorithm that can do similar things to a sufficiently good level? Am I to believe that hardware is not going to duplicate this? Can people just build a giant vat of neurons and get way better results out of an analogous quantum algorithm? When obstacles are discovered, AI people

are clever and look for ways around the obstacles. So

it would extend the timeline, but it wouldn't extend it by 50 years.

"As a neuroscientist, can I look for analogues of value alignment problems in my own work, and if so, how?" That's a new question. If I'm not allowed to take five minutes to go quiet and think about it, then my immediate answer is: it's not obvious where the analogues would be, unless there was something equivalent to maybe conservatism or ambiguity identification.

It's natural selection that aligns mammals, more than — it's not like mammals are aligned to outside systems using a simple alignment algorithm that is loading the values from the outside system. We come with it wired into our brains already. The part where natural selection caused the particular goals we pursue to align in the ancestral environment with inclusive genetic fitness has already been done. Plus, natural selection completely botched it. Humans do not pursue inclusive genetic fitness under reflection. We were just a particular kind of thing that operated to coincidentally produce inclusive genetic fitness in the ancestral environment. And once we got access to contraceptives, we started using them. If there's a lesson to derive from natural selection, it would be something along the lines of: if you have a Turing-complete thing you are optimizing — such as DNA, not literally Turing-complete because can't get arbitrarily big, but you know what I mean — and you apply enough optimization pressure to this Turing-complete program to make it pursue a goal like inclusive genetic fitness, you will get a thing that is actually a sapient consequentialist deliberately planning how to get a bunch of stuff that isn't actually that thing. We are the demons of natural selection. We are the optimization systems that popped up inside the optimization system in a way that was unanticipated — if natural selection could anticipate anything. The main lesson we have to draw from natural selection is: don't do it that way. And there might be lessons that we can draw from looking at the brain that are going to play a role in value alignment theory. But aside from looking at particular problems and asking, "Is there a thing in the brain that does conservatism? Is there a thing in the brain that does ambiguity identification?" — it's not clear to me that there's any principled answer for how you could take stuff from neuroscience and import into value alignment.

"If you have technical solutions, how do you get AI people to implement them?" Stuart

Russell is, I think, the main person who, as an insider, making the principled appeal. You do not have bridge engineering and then a bunch of people outside who aren't engineers thinking about how to have bridges not fall down. The problem of bridge engineering just is: make a bridge that doesn't fall down. The problem of AGI, we should see as just: how do you run computer programs that produce beneficial effects in the environment? Where the fact that you're trying to direct it toward a particular goal assumed — the way that when you're trying to build a chess device, the fact that you're trying to direct it particular goal assumed. Not "how do we rush frantically to get something, anything, with intelligence?" So there's that line of pursuit. The Future of Humanity Institute at Oxford does a lot of public-facing work. The Machine Intelligence Research Institute, where I work, sees its own role as being more like: make sure that the technical stuff is there to back the people saying "do this right" on a technical level. So I don't actually have the expertise to answer your question as well as I might like, because we're the ones who specialize in sort of going off and trying to solve the technical problems, while FHI, in addition to doing some technical problems, also does public-facing stuff. That said, there certainly have been disturbing trends in this area. And I think we're starting from a rather low baseline of concern, where

startups have been telling venture capitalists that they will have AGI for a long time before the first time any of them ever said, "We will have AGI and it will not destroy the world."

The very thought that you need to point these things in a direction, and that this is actually an interesting technical part of the problem that you actually need to solve and be careful about, is new and does need to be propagated. Next.

"Can you a little bit more depth on conservatism?"

So first, conservatism has nothing to do with the political movement, one way or another. That said, it's the sort of thing that recently opened up, where we just started to issue calls for proposals and put up various things on whiteboards and stare at them. An example of a thing that we recently started on the whiteboard: somebody said, "Well, suppose that you do have multiple hypotheses for what the classification rule could be. Is there any difference between the form of conservatism we want and maximizing the probability that something is inside the class, given that you have multiple hypotheses? The point of maximum probability will be at the point of maximum overlap." And I waved my hands a bit and said, "Well, it seems to me that these two could come apart, because you could have exceptionally simple classifiers that

imply increased probabilities in a particular portion of the space, and so you might just end up over there in this weird corner of the space that does maximum probability, whereas things that humans actually want are going to be classified according to a more complicated rule that's not going to be very close to the start of the list of potential classification rules." It does seem to me that on a conceptual level, maximizing probability seems like we might very well be asking for a different thing than "classify this well while covering as little territory as possible." But basically it's a very new question, and we haven't done that much real work on it — just more phrasing questions and answering questions at this point, I think.

"In

generating smiles, there was a step where the AGI wide enough to come with something the with.

Given that we solve the utility indifference problem, would that be a good path to go — try to figure out how to switch it off whenever it comes kind of like a modest version of that?" So the question is: there was a step in the story told before where the AGI started working out what behaviors its programmers would not want to see and avoiding those behaviors, so as to appear — from our perspective — deceptively nice, and from its perspective, continuing to get maximum expected value for its utility function. Could the switch-between-utility-functions algorithm be a way to work around or avoid that scenario? Yes. It is. The switching between two utilities on or off is indeed the basic case of "learn a more complicated utility by watching the environment without trying to tamper with the data that the environment is giving you." Great question. The answer is yes. Next question.

"How do you check your own assumptions? How do you make sure that you're not just looking at a subset of problems?" So the question is: you seem to detect sort of humanoid or anthropomorphic assumptions. How do you check those? How do you make sure you're not restricting yourself to a tiny section of the space?

It's very hard to know that you're not thinking like a human from the perspective of an AI. I did sort of start to give an example of a case where it seems like we might be able to say utility functions — and, by very similar arguments, coherent probability distributions — are things that start to come up in sufficiently advanced agents, because we have multiple coherence theorems all pointing in the same direction, at the same class of behaviors. And you can't actually do perfect expected utility maximization, because you can't evaluate every outcome. What you can say is something like: to the extent that you, as a human, can predict behavior that is incompatible with any utility function, you are predicting a stupidity of the system. So a system that has stopped being stupid from your perspective will look to you as if it is compatible with having a utility function, as far as you can tell in advance. So that was an instance of trying to give an argument that goes past the human. In a lot of cases where I talk about an AI potentially

modeling its programmers and avoiding behavior that it expects to lead to its utility function being edited, this is just me putting myself in the AI's shoes. But for a sufficiently advanced agent, we can make something like an efficiency assumption. An efficient market price is not an accurate market price; it's market price such that you can't predict a net change in that price. Suppose I asked you to imagine a superintelligence trying to estimate the number of hydrogen atoms in the Sun. We don't expect it to get the number exactly right, but if you think that you can say in advance, "Oh, it's going to forget that hydrogen atoms are very light and underestimate the number by 10%," you're proposing something that is akin to predicting that Microsoft stock price will rise by 10% over the next week without using insider information. You are proposing that you know a directional error in the estimates of the other agent. Similarly, we can look at a modern chess program, which is now way above the human level, and say, "Okay, I think the chess program will move over here in order to pursue a checkmate." You could be right. Suppose that the program moves somewhere else. Do we say, "Haha, didn't take the best move"? No. We say, "Whoops, I guess I was wrong about what the best move was." We suppose that either we

overestimated how much expected utility was available from the move we thought it would take, or we underestimated the expected utility available from a different move. And the more surprising the other move is, the more we think we've underestimated that move. So

if you ask me, "Will the AI actually be modeling the programmers?" or "Will it actually go for protein folding to get nanotechnology?" — first of all, it might not apply to an AI that is not strictly superhuman. But second, if it is sufficiently superhuman that I don't expect it to do that exact thing, I'm in a state of Vingean uncertainty. It's smarter than me. I can't predict its exact policy. But I expect it to get at least as much expected utility as I could get in its shoes. So if it's not pursuing molecular nanotechnology — given that Eric Drexler in the book Nanosystems ran numerous basic calculations strongly indicating feasibility, nanotechnology looks like it should be possible, and in a certain sense it is possible, it's in all of us, and it's held together by weak little van der Waals instead of covalent bonds; we can have things that are to

ribosomes as steel is to flesh — so maybe an AI doesn't pursue that. But if so, it's because it found something better, not because it's just leaving the value on the table from its perspective. I am out of time. Therefore this talk is now over. [Applause]
