---
title: "Tech is Good, AI Will Be Different"
channel: "Robert Miles AI Safety"
url: "https://www.youtube.com/watch?v=zATXsGm_xJo"
---

Hi, this channel is about the risks of advanced AI and the safety research that people are doing to mitigate those risks. And sometimes when people hear that, they say to me, "But Rob, technology is good." People are always scared of new technology, usually for pretty bad reasons.

[Video clip]

Technology. Oh my god.

[End clip]

And their safety concerns slow new technologies down. So why are you worried about AI risks? Don't you realize that technology is good?

And the thing about this is, it's true. Technology is good, and fears about new technology usually are misplaced. I think advanced AI may be an exception to the rule. Can you guess why? But the rule itself is very real and very important to me. I'm pretty much always on that side of these kinds of debates and have been for a long time. In fact, I'm probably more pro-technology than most of the people who criticize me on this.

Like, okay, I try not to give political opinions outside of my direct area of expertise, but since you asked, I think we should probably abolish the FDA and replace it with something that doesn't slow down medical progress so much. The current policy is effectively all new medicine is banned by default, which is pretty wild if you think about it. Obviously, you need rules and regulations about medicine, and I'm not an expert. I'm not going to claim to know exactly what system would work best. But my point is, I think safety concerns are slowing down medical progress too much, and we should prioritize that more.

We should probably also relax research ethics a bit. Like, for example, we should have way more human challenge trials. We'd get new treatments much quicker. I also think the commercial aerospace industry is too conservative because of safety concerns. Airplanes are extremely safe relative to basically any other form of transport, which is great, but like, we could have had supersonic passenger aircraft years ago. I mean, practical, economically viable supersonic passenger aircraft. Instead, we kind of went backwards. Hopefully, that's changing soon.

And we could have built enough nuclear power to be basically done with fossil fuel power generation decades ago as well, but nuclear power is held back by safety concerns in most of the world. So we're still burning coal, which releases more harmful radiation per kilowatt hour than nuclear does. Same thing with self-driving cars. They're already safer than human drivers who kill about a million people every year. Each year that we delay these kinds of things effectively kills a large number of people. It doesn't make sense to delay them for safety reasons.

I want a cure for cancer, a self-driving car powered by cheap nuclear energy, and a jetpack, damn it. Okay, well, the jetpack might actually be unsafe, but the rest would be good. Technology is good. Technology is the main cause of the enormous improvements in living standards that we've achieved throughout history. And I mean, just look at it. Look at how cool this is.

[Video montage]

Obviously, technology is good, right? Well, of course, this is not obvious to everyone, so we should probably make an actual argument. Why is technology good? Like, if you think about it, it is a bit strange to be able to say that something as broad as technology is good. Technology is a huge range of stuff, including good and bad. Why should we expect such a wide range of things to be good overall? This actually is not obvious.

My answer is technology is good because people are good. Now, if you don't think people are good, I'm sorry, but this is a staunchly pro-humanity channel. If that bothers you, you can leave. You're very welcome to stay though either way, because you're a person and therefore probably good. But yeah, technology is good because people are good.

Technology enables people to do new things. It gives them new options and new capabilities. People use those new capabilities to do what they want, to get more of what they want. Since most people are basically good and mostly want good things, people being able to do more stuff to get what they want ends up being good overall. In fact, allowing people to get what they want is a very important principle, which you may also know by the name freedom.

If you want to stop someone from getting what they want, you'd better have a good reason. And the same applies to standing in the way of a new technology. Of course, sometimes some relatively small number of people want something bad and use technology to get it, right? New technology can be misused, but most people are good, and people who want to do harm tend to be outnumbered and outgunned. Often a bigger problem is unintended side effects or accidents. These can be pretty bad, but the technology is often still overall worth it, and we can deal with problems as they arise. And usually these problems are dealt with not by trying to go backwards, but by new and better technology.

Okay, so if I think that technology is good and is too often slowed down by safety concerns, what am I doing running a channel about my AI safety concerns? Why is AI any different from other technology? The answer is it isn't really, or rather, it isn't yet, but it will be sooner or later for a few reasons.

Firstly, advanced general-purpose artificial intelligence is the first technology that can function as a capable independent agent with its own goals. As I said, the reason technology is good is because it's a tool to get people what they want. But, and this is important because this makes AGI fundamentally different from all previous technology in history, once you have AI systems intelligently pursuing their own goals, you have the first ever technology which isn't just about enabling people to get what they want, but about the technology itself getting what it wants. And what does it want? Well, it would be cool if we had some reliable way of knowing that or updating it or making sure it's what we intended. Technology is good. AI will be different.

And that difference has a lot of effects. Perhaps most importantly, the normal methods we use for other technology, where we deal with each new problem as it comes up, just aren't up to the task of dealing with AGI by default. Advanced AI can hide problems from us, manipulate us, and interfere with our attempts to fix things. Our standard safety methods and societal processes are not set up to deal with technological systems that can intelligently act against us.

Like, we invented CFCs. They made a hole in the ozone layer. We developed new refrigerants that didn't have that problem and mandated their use, and now the ozone hole is closing. CFCs weren't able to hide the hole in the ozone layer or pin the blame on some other kind of molecule. CFCs are just simple chemical compounds.

Similarly, we invented planes. They crashed. We figured out why and improved the technology. They crashed again. We fixed those problems. They crashed again. We repeated this an honestly kind of embarrassingly large number of times, and now planes are extremely safe. But planes aren't able to notice when they're being tested and behave differently to make sure they pass the tests so that they can make sure to crash only when they're carrying real passengers. None of the people involved wants that, and planes are just tools.

In the same way, we invented nuclear power. We had Chernobyl and Three Mile Island. We improved the technology, and now nuclear power is much safer than fossil fuels and should have been powering everything for a while now. But nuclear power plants aren't able to carefully observe our safety protocols in order to decide the best way to melt down and the best time to do it so that it's least likely to be contained. Power plants aren't agents. AGI is different from all other technology.

And notice how in all of those examples, the hazard did have to actually happen, often several times, and we responded to it after that. We did have a nuclear power plant actually melt down. We did have a ton of planes actually crash and kill real people, and we fixed the problems once they'd already happened. AGI may not give us so many chances.

The regular approach where you have the full-blown form of the hazard happen and then you learn from it, that only works if the hazard is recoverable. AGI risks are not necessarily recoverable. If we lose control of the world, we've lost it permanently. We shouldn't expect to be able to get it back from an intelligent adversary.

Has humanity ever managed to rein in a large-scale risk before the full-blown form of the hazard has actually happened at least once? Kind of with nuclear weapons. We had some very close calls, but we've so far just about managed to avoid having a global thermonuclear war. How did we achieve that? Well, we developed the new technology. We saw that a potentially unrecoverable outcome was possible, and then we freaked out and did unprecedented things to prevent it from happening.

In that case, humanity correctly recognized that there are certain risks that you can't have happen even once because they're not recoverable. We can have a pretty big global pandemic and come back from it. We can even come back from a medium-sized nuclear exchange. But we can't have AI take over the world one time, say, "Oops," figure out what went wrong, learn from our mistake, and move on. Once we lose, we've lost. We can't be driven extinct and then patch the issue in the next release.

Technology is good, and it usually shouldn't be slowed down too much by safety concerns. This is true of technology broadly, including almost all AI technology up until this point. But advanced artificial general intelligence will be categorically different from every other technology in history, and we need to recognize that and act accordingly.

[Music]

Hey, do you know about AIsafety.info? It's this great website that grew out of my Discord server. It's awesome. They've got answers to hundreds of the most common questions about AI safety. There's even a chatbot you can talk to. It's very cool, and it's a great place to send new people. AIsafety.info. Check it out.

Also, if you're looking to help out, they do pretty urgently need funding to keep the thing running. Speaking of which, I'm kept running by my wonderful patrons, all of these amazing people here. Thank you all so much. In this video, I'm especially thanking Allied Toasters. So thank you, Allied Toasters, and thank you all for watching. I'll see you next time.

[Music]
