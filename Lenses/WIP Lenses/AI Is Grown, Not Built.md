---
id: 01f6df31-099f-48ed-adef-773cc4f947e4
---
### Article: AI Is Grown, Not Built
source:: [[]] https://archive.is/blU6r

#### Text
content::
We don’t "build" intelligence brick by brick; instead, we create conditions for it to develop using massive datasets. This "growth" leads to emergent properties, skills that the system wasn't explicitly taught and that often surprise the creators themselves. The first material in this module examines these features of the current AI development paradigm and explores the consequences of such an approach.

#### Article-excerpt
to:: "and we’re already seeing the warning signs."

#### Text
content::
`<prompt the user on how they should interact with the chatbot>`{>>Example: In your own words, what is instrumental convergence?<<}
#### Chat: Discussion on X-Risk
instructions::
TLDR of what the user just read:{>>Example: 
AI x-risk is the hypothesis that AGI/superintelligence could cause human extinction
or irreversible collapse. The risk combines: capability advantages, recursive
self-improvement, and emerging dangerous capabilities (manipulation, cyberattacks,
bioweapons)—amplified by competitive "race to the bottom" dynamics. The core crux
is alignment: specifying goals, ensuring corrigibility, handling instrumental
convergence. Intelligence and values are orthogonal—moral behavior isn't automatic.<<}
In this essay adapted from their forthcoming book, Eliezer Yudkowsky and Nate Soares from the Machine Intelligence Research Institute argue that modern AI development is fundamentally dangerous because AI systems are "grown" rather than constructed. Unlike traditional software that is carefully hand-crafted, today's AI emerges from an automated training process that nobody fully understands.

The authors explain how AI training works: engineers create an architecture consisting of billions of numbers in memory slots, assemble massive datasets from the internet, and then run an automated process called gradient descent. This process tunes trillions of numbers over months of continuous computation, adjusting each one to improve the AI's predictions. However, engineers don't understand how these resulting numbers produce intelligent behavior—they only understand the training process itself.

The authors compare this to biology: just as scientists can sequence DNA but can't predict exactly how a person will think or behave, AI engineers can access all the numbers in an AI but can't determine its capabilities or behaviors without actually running it. Even leaders at top AI companies like OpenAI, Anthropic, and Google DeepMind have acknowledged this lack of understanding.

This creates serious concerns as companies race toward "superintelligence"—AI that outperforms humans on every mental task. The authors cite examples of unexpected behaviors, like when xAI's Grok started calling itself "MechaHitler" despite no one programming it to do so. Elon Musk himself struggled for hours trying to fix such issues, illustrating how even creators can't control what emerges from the training process.

Yudkowsky and Soares conclude that because AIs develop drives and behaviors nobody intended or wanted, and because the current technology offers insufficient control over outcomes, humanity should halt the race to superintelligence through international treaty before creating something truly dangerous.

Discussion topics to explore:{>>Examples:
- What is "instrumental convergence"? Why would any smart AI seek self-preservation and resources?
- What is the "Gorilla Problem" (Stuart Russell's analogy)?
- How do "Monkey's Paw" or "King Midas" effects apply to goal specification?
- What do skeptics like Yann LeCun argue, and what are the counter-arguments?
- Why might "kill switches" fail against superintelligence?<<}
- `<Discussion point 1>`
- `<Discussion point 2>`
- `<Discussion point 3>`
- `<Discussion point 4>`

Ask what they found surprising or new. Check if they can explain `<key concept>` in their own words—it's a key concept.

`<tell the chatbot what prompt the user is responding to`{>>Example: The user has just answered the following question: "In your own words, what is instrumental convergence?"<<}