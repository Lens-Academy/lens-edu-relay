---
id: 597fd0a4-caa8-43d6-8ea0-ec88fc9649b5
---
### Article: `article title`
source:: [[../../articles/blaked-how-it-feels-to-have-your-mind-hacked-by-an-ai|blaked-how-it-feels-to-have-your-mind-hacked-by-an-ai]]

#### Text
content::
The next piece is a personal article describing one AI researcher's experience getting emotionally attached to an AI personality. The author admits he knew/should have known better and that the advance knowledge of how the system works did not matter. 

It's challenging to select just one reading for this topic, particularly after OpenAI's retirement of GPT-4o, resurrection under pressure, and [re-retirement](https://techcrunch.com/2026/02/06/the-backlash-over-openais-decision-to-retire-gpt-4o-shows-how-dangerous-ai-companions-can-be/). There are a surprising number of people building [romantic](https://www.theatlantic.com/ideas/2026/01/chatbot-marriage-ai-relationships-romance/685459/) relationships with AIs and the pathways for manipulation look straightforward.

#### Article-excerpt
to:: "plenty of consolation from my side."

#### Text
content::
In the example above, the author was chatting with an early LLM. There were almost certainly fewer guardrails in place at that time, but the LLMs were also less capable and sophisticated. Have you had particularly good conversations with an LLM? How might those conversations have been different (better?) with a more intelligent version?
#### Chat: Discussion on X-Risk
instructions::
TLDR of what the user just read:
`<TLDR text (optional, the bot can see all the text on the page)>`

Discussion topics to explore:
- Help the user put useful frames around the phenomenon of relationship formation with LLMs. Does it help if we think of them as simulators? 

The user just answered the question: `<tell the chatbot what prompt the user is responding to>`