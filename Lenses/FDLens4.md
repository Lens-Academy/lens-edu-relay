---
id: e81a74a1-5f59-44d7-bf1f-e6f0bb48d834
tags :
  - lens
  - work-in-progress
---
### Article: A central AI alignment problem – capabilities generalization, and the sharp left turn by Nate Soares
source:: [[../a]]

#### Text
content::
The central challenge in alignment is ensuring that the learned goal remains stable as the system becomes more powerful. This article explores the phenomenon of capabilities generalisation: a situation where an AI learns a skill in a limited environment but applies it in unintended ways when faced with broader challenges. Researchers call this the sharp left turn scenario. It is a moment when an AI's internal logic shifts rapidly, potentially leading to a misalignment that was invisible during the training phase.

If we cannot predict how an AI’s objectives will evolve as its capabilities grow, we are fundamentally unable to guarantee its safety. This material examines why capabilities tend to generalise much faster than alignment, creating a dangerous gap in our control over the system.

#### Article-excerpt
to:: "`<an exact quote from the article where the exerpt should stop>`"

#### Text
content::
`<prompt the user on how they should interact with the chatbot>`
#### Chat: Discussion on X-Risk
instructions::
TLDR of what the user just read:
`<TLDR text (optional, the bot can see all the text on the page)>`

Discussion topics to explore:
- `<Discussion point 1>`
- `<Discussion point 2>`
- `<Discussion point 3>`
- `<Discussion point 4>`

Ask what they found surprising or new. Check if they can explain `<key concept>` in their own words—it's a key concept.

The user just answered the question: `<tell the chatbot what prompt the user is responding to>`