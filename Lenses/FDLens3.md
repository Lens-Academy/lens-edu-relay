---
id: 8be5ec8c-df65-4423-aa3f-5776384a1c41
tags :
  - lens
  - work-in-progress
---
### Article: When should we worry about AI power-seeking by Joe Carlsmith
source:: [[../articles/carlsmith-when-should-we-worry-about-ai-power-seeking]]

#### Text
content::
Power-seeking is a theoretical prediction based on rational agent models. Humans seek power due to evolutionary pressures. AI might seek power due to optimization pressures. The Instrumental Convergence thesis assumes that AI agents will be highly coherent and strategic. If the system behaves like an agent, e.g., seeks to maximize the probability of success, it is mathematically inclined to choose paths that preserve or increase the number of available options, and that is how power-seeking behaviour emerges. It is possible that future models will be agentic enough to show this predicted power-seeking behaviour but the risk depends heavily on how incentives are shaped and how capable the system becomes. This material examines the conditions required for power-seeking AI behaviour.

#### Article-excerpt
to:: "here to safe, useful superintelligent AI agents."

#### Text
content::
`<prompt the user on how they should interact with the chatbot>`
#### Chat: Discussion on X-Risk
instructions::
TLDR of what the user just read:
`<TLDR text (optional, the bot can see all the text on the page)>`

Discussion topics to explore:
- `<Discussion point 1>`
- `<Discussion point 2>`
- `<Discussion point 3>`
- `<Discussion point 4>`

Ask what they found surprising or new. Check if they can explain `<key concept>` in their own wordsâ€”it's a key concept.

The user just answered the question: `<tell the chatbot what prompt the user is responding to>`